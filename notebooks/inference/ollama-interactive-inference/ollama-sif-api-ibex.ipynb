{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ef82580-be33-446a-a789-f2ce06a6989d",
   "metadata": {},
   "source": [
    "# OLLAMA - REST API Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65906f71",
   "metadata": {},
   "source": [
    "> This page was generated from [ollama-interactive-inference/ollama-sif-api-ibex.ipynb](https://github.com/kaust-rccl/Data-science-onboarding/tree/main/notebooks/inference/ollama-interactive-inference/ollama-sif-api-ibex.ipynb). You can [view or Download notebook](https://github.com/kaust-rccl/Data-science-onboarding/tree/main/notebooks/inference/ollama-interactive-inference/ollama-sif-api-ibex.ipynb). Or [view it on nbviewer](https://nbviewer.org/github/kaust-rccl/Data-science-onboarding/tree/main/notebooks/inference/ollama-interactive-inference/ollama-sif-api-ibex.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494d3541",
   "metadata": {},
   "source": [
    "## Objective\n",
    "In this notebook, we are going to use Ollama using REST API approach \n",
    "\n",
    "## Initial Setup\n",
    "If you haven't installed conda yet, please follow [`How to Setup Conda on Ibex Guide`](https://docs.hpc.kaust.edu.sa/soft_env/prog_env/python_package_management/conda/ibex.html) to get started.\n",
    "\n",
    "After conda has been installed, save the following environment yaml file on Ibex under the name *ollama_env.yaml*\n",
    "\n",
    "```yml\n",
    "name: ollama_env\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - _libgcc_mutex=0.1\n",
    "  - _openmp_mutex=4.5\n",
    "  - _python_abi3_support=1.0\n",
    "  - anyio=4.11.0\n",
    "  - argon2-cffi=25.1.0\n",
    "  - argon2-cffi-bindings=25.1.0\n",
    "  - arrow=1.4.0\n",
    "  - asttokens=3.0.0\n",
    "  - async-lru=2.0.5\n",
    "  - attrs=25.4.0\n",
    "  - babel=2.17.0\n",
    "  - beautifulsoup4=4.14.2\n",
    "  - bleach=6.2.0\n",
    "  - bleach-with-css=6.2.0\n",
    "  - brotli-python=1.1.0\n",
    "  - bzip2=1.0.8\n",
    "  - ca-certificates=2025.10.5\n",
    "  - cached-property=1.5.2\n",
    "  - cached_property=1.5.2\n",
    "  - certifi=2025.10.5\n",
    "  - cffi=2.0.0\n",
    "  - charset-normalizer=3.4.4\n",
    "  - comm=0.2.3\n",
    "  - cpython=3.14.0\n",
    "  - debugpy=1.8.17\n",
    "  - decorator=5.2.1\n",
    "  - defusedxml=0.7.1\n",
    "  - exceptiongroup=1.3.0\n",
    "  - executing=2.2.1\n",
    "  - fqdn=1.5.1\n",
    "  - h11=0.16.0\n",
    "  - h2=4.3.0\n",
    "  - hpack=4.1.0\n",
    "  - httpcore=1.0.9\n",
    "  - httpx=0.28.1\n",
    "  - hyperframe=6.1.0\n",
    "  - idna=3.11\n",
    "  - importlib-metadata=8.7.0\n",
    "  - ipykernel=7.0.1\n",
    "  - ipython=9.6.0\n",
    "  - ipython_pygments_lexers=1.1.1\n",
    "  - isoduration=20.11.0\n",
    "  - jedi=0.19.2\n",
    "  - jinja2=3.1.6\n",
    "  - json5=0.12.1\n",
    "  - jsonpointer=3.0.0\n",
    "  - jsonschema=4.25.1\n",
    "  - jsonschema-specifications=2025.9.1\n",
    "  - jsonschema-with-format-nongpl=4.25.1\n",
    "  - jupyter-lsp=2.3.0\n",
    "  - jupyter_client=8.6.3\n",
    "  - jupyter_core=5.9.1\n",
    "  - jupyter_events=0.12.0\n",
    "  - jupyter_server=2.17.0\n",
    "  - jupyter_server_terminals=0.5.3\n",
    "  - jupyterlab=4.4.9\n",
    "  - jupyterlab_pygments=0.3.0\n",
    "  - jupyterlab_server=2.27.3\n",
    "  - keyutils=1.6.3\n",
    "  - krb5=1.21.3\n",
    "  - lark=1.3.0\n",
    "  - ld_impl_linux-64=2.44\n",
    "  - libedit=3.1.20250104\n",
    "  - libexpat=2.7.1\n",
    "  - libffi=3.4.6\n",
    "  - libgcc=15.2.0\n",
    "  - libgcc-ng=15.2.0\n",
    "  - libgomp=15.2.0\n",
    "  - liblzma=5.8.1\n",
    "  - libmpdec=4.0.0\n",
    "  - libsodium=1.0.20\n",
    "  - libsqlite=3.50.4\n",
    "  - libstdcxx=15.2.0\n",
    "  - libstdcxx-ng=15.2.0\n",
    "  - libuuid=2.41.2\n",
    "  - libzlib=1.3.1\n",
    "  - markupsafe=3.0.3\n",
    "  - matplotlib-inline=0.1.7\n",
    "  - mistune=3.1.4\n",
    "  - nbclient=0.10.2\n",
    "  - nbconvert-core=7.16.6\n",
    "  - nbformat=5.10.4\n",
    "  - ncurses=6.5\n",
    "  - nest-asyncio=1.6.0\n",
    "  - notebook-shim=0.2.4\n",
    "  - openssl=3.5.4\n",
    "  - overrides=7.7.0\n",
    "  - packaging=25.0\n",
    "  - pandocfilters=1.5.0\n",
    "  - parso=0.8.5\n",
    "  - pexpect=4.9.0\n",
    "  - pickleshare=0.7.5\n",
    "  - pip=25.2\n",
    "  - platformdirs=4.5.0\n",
    "  - prometheus_client=0.23.1\n",
    "  - prompt-toolkit=3.0.52\n",
    "  - psutil=7.1.0\n",
    "  - ptyprocess=0.7.0\n",
    "  - pure_eval=0.2.3\n",
    "  - pycparser=2.22\n",
    "  - pygments=2.19.2\n",
    "  - pysocks=1.7.1\n",
    "  - python=3.14.0\n",
    "  - python-dateutil=2.9.0.post0\n",
    "  - python-fastjsonschema=2.21.2\n",
    "  - python-gil=3.14.0\n",
    "  - python-json-logger=2.0.7\n",
    "  - python-tzdata=2025.2\n",
    "  - python_abi=3.14\n",
    "  - pytz=2025.2\n",
    "  - pyyaml=6.0.3\n",
    "  - pyzmq=27.1.0\n",
    "  - readline=8.2\n",
    "  - referencing=0.37.0\n",
    "  - requests=2.32.5\n",
    "  - rfc3339-validator=0.1.4\n",
    "  - rfc3986-validator=0.1.1\n",
    "  - rfc3987-syntax=1.1.0\n",
    "  - rpds-py=0.27.1\n",
    "  - send2trash=1.8.3\n",
    "  - setuptools=80.9.0\n",
    "  - six=1.17.0\n",
    "  - sniffio=1.3.1\n",
    "  - soupsieve=2.8\n",
    "  - stack_data=0.6.3\n",
    "  - terminado=0.18.1\n",
    "  - tinycss2=1.4.0\n",
    "  - tk=8.6.13\n",
    "  - tomli=2.3.0\n",
    "  - tornado=6.5.2\n",
    "  - traitlets=5.14.3\n",
    "  - typing-extensions=4.15.0\n",
    "  - typing_extensions=4.15.0\n",
    "  - typing_utils=0.1.0\n",
    "  - tzdata=2025b\n",
    "  - uri-template=1.3.0\n",
    "  - urllib3=2.5.0\n",
    "  - wcwidth=0.2.14\n",
    "  - webcolors=24.11.1\n",
    "  - webencodings=0.5.1\n",
    "  - websocket-client=1.9.0\n",
    "  - yaml=0.2.5\n",
    "  - zeromq=4.3.5\n",
    "  - zipp=3.23.0\n",
    "  - zstandard=0.25.0\n",
    "  - zstd=1.5.7\n",
    "  - pip:\n",
    "      - annotated-types==0.7.0\n",
    "      - ollama==0.6.0\n",
    "      - pydantic==2.12.3\n",
    "      - pydantic-core==2.41.4\n",
    "      - typing-inspection==0.4.2\n",
    "```\n",
    "\n",
    "Run the following command to build the conda environment:\n",
    "```bash\n",
    "conda env create -f ollama_env.yaml\n",
    "```\n",
    "\n",
    "## Starting JupyterLab\n",
    "Follow [`Guide: Using Jupyter on Ibex`](https://docs.hpc.kaust.edu.sa/soft_env/job_schd/slurm/interactive_jobs/jupyter.html#job-on-ibex) to start JupyterLab on a an Ibex GPU node using your conda environment instead of *'machine_learning'* module.\n",
    "\n",
    "By making the following changes to the Jupyter launch script:\n",
    "```bash\n",
    "#module load machine_learning/2024.01\n",
    "conda activate ollama_env\n",
    "```\n",
    "\n",
    "## Starting The Ollama Server\n",
    "Start the OLLAMA REST API server using the following bash script in a terminal:\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "# Cleanup process while exiting the server\n",
    "cleanup() {\n",
    "    echo \"üßπ   Cleaning up before exit...\"\n",
    "    # Put your exit commands here, e.g.:\n",
    "    rm -f $OLLAMA_PORT_TXT_FILE\n",
    "    # Remove the Singularity instance\n",
    "    singularity instance stop $SINGULARITY_INSTANCE_NAME\n",
    "}\n",
    "trap cleanup SIGINT  # Catch Ctrl+C (SIGINT) and run cleanup\n",
    "\n",
    "# User Editable Section\n",
    "# 1. Make target directory on /ibex/user/$USER/ollama_models_scratch to store your Ollama models\n",
    "export OLLAMA_MODELS_SCRATCH=/ibex/user/$USER/ollama_models_scratch\n",
    "mkdir -p $OLLAMA_MODELS_SCRATCH\n",
    "# End of User Editable Section\n",
    "\n",
    "SINGULARITY_INSTANCE_NAME=\"ollama\"\n",
    "OLLAMA_PORT_TXT_FILE='ollama_port.txt'\n",
    "\n",
    "# 2. Load Singularity module\n",
    "module load singularity\n",
    "\n",
    "# 3. Pull OLLAMA docker image\n",
    "singularity pull docker://ollama/ollama\n",
    "\n",
    "# 4. Change the default port for OLLAMA_HOST: (default 127.0.0.1:11434)\n",
    "export PORT=$(python -c 'import socket; s=socket.socket(); s.bind((\"\", 0)); print(s.getsockname()[1]); s.close()')\n",
    "\n",
    "# 5. Copy the assigned port, it will be required in the second part during working on the notebook.\n",
    "echo \"$PORT\" > $OLLAMA_PORT_TXT_FILE\n",
    "\n",
    "echo \"OLLAMA PORT: $PORT  -- Stored in $OLLAMA_PORT_TXT_FILE\"\n",
    "\n",
    "# 6. Define the OLLAMA Host\n",
    "export SINGULARITYENV_OLLAMA_HOST=127.0.0.1:$PORT\n",
    "\n",
    "# 7. Change the default model directory stored: (default ~/.ollama/models/manifests/registry.ollama.ai/library)\n",
    "export SINGULARITYENV_OLLAMA_MODELS=$OLLAMA_MODELS_SCRATCH\n",
    "\n",
    "# 8. Create an Instance:\n",
    "singularity instance start --nv -B \"/ibex/user:/ibex/user\" ollama_latest.sif $SINGULARITY_INSTANCE_NAME\n",
    "\n",
    "# 7. Run the OLLAMA REST API server on the background\n",
    "singularity exec instance://$SINGULARITY_INSTANCE_NAME bash -c \"ollama serve\"\n",
    "```\n",
    "\n",
    "> Note: Save the above script in a file called start_ollama_server.sh\n",
    "\n",
    "```bash\n",
    "# Run the script to start the Ollama server.\n",
    "bash start_ollama_server.sh\n",
    "```\n",
    "\n",
    "The script has the following:\n",
    "- A user editable section, where the user defines Ollama models scratch directory.\n",
    "- The allocated port is saved in a temporary ollama_port.txt file, in order to be used in the Python notebook to read the assigned port to Ollama server.\n",
    "- Cleanup section in order to stop the singularity instance when the script is terminated with CTRL+C.\n",
    "\n",
    "## Using REST API Requests\n",
    "Follow the following Python notebook below, it contains the codes for:\n",
    "- Initialization Setup.\n",
    "- List local models.\n",
    "- Pull models.\n",
    "- Testing connection to the Ollama server.\n",
    "- Chat with the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107ad806",
   "metadata": {},
   "source": [
    "### 1. Initialization\n",
    "1. Define the base URL for the remote Ollama Server.\n",
    "2. Testing the Ollama server connectivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e5a36c6-1cd6-4c79-8cc5-274b4d51dced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://127.0.0.1:51023\n"
     ]
    }
   ],
   "source": [
    "# 1.1- Define the base URL for the remote Ollama Server\n",
    "with open(\"ollama_port.txt\") as f :\n",
    "    PORT = f.read().strip()\n",
    "    \n",
    "BASE_URL=f\"http://127.0.0.1:{PORT}\"\n",
    "print(BASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e4f918a-1880-481d-b3d3-542a9427131e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama is running! 200\n"
     ]
    }
   ],
   "source": [
    "# 1.2- Testing the Ollama server connectivity\n",
    "import requests\n",
    "\n",
    "try:\n",
    "    r = requests.get(BASE_URL)\n",
    "    print(\"Ollama is running!\", r.status_code)\n",
    "except requests.ConnectionError as e:\n",
    "    print(\"Ollama is NOT reachable:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31f5755-a214-4457-ad51-cb2879e93f97",
   "metadata": {},
   "source": [
    "### 2. Get a List of Local Models\n",
    "- Get a list of locally available Ollama models.\n",
    "- Locally available models are located under path: */ibex/user/$USER/ollama_models_scratch*\n",
    "- To change the location for pulled models, modify the variable *OLLAMA_MODELS_SCRATCH* in the script*start_ollama_server.sh*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4293ef11-0d95-43c6-b9f8-39342dcae500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gemma3:270m', 'qwen3:0.6b']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a list of downloaded models\n",
    "def get_local_models(base_url: str = BASE_URL):\n",
    "    \"\"\"\n",
    "    Returns a list of locally available Ollama Models.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of model names as strings\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError: If there is a failure to connect the Ollama server.\n",
    "    \"\"\"\n",
    "    r = requests.get(f\"{base_url}/api/tags\")\n",
    "    if r.ok:\n",
    "        models = r.json().get(\"models\", [])\n",
    "        return [m[\"name\"] for m in models]\n",
    "    else:\n",
    "        raise RuntimeError(f\"Failed to list models: {r.text}\")\n",
    "\n",
    "get_local_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627a45b1-f2c1-4088-a190-a9d5b70e5630",
   "metadata": {},
   "source": [
    "### 3. Pull The Model\n",
    "- Pull a model from the Ollama server and stream the download progress.\n",
    "- Please refer to [Ollama Library](https://ollama.com/library) to check available models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20cd9a8f-ebb7-4ec3-92d3-eb516a824569",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pull the required model\n",
    "import requests\n",
    "\n",
    "def pull_model(model: str, base_url: str =BASE_URL) -> list:\n",
    "    \"\"\"\n",
    "    Pull a model from the Ollama server and stream the download progress.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the model to pull.\n",
    "        base_url (str, optional): Base URL of the Ollama server. Defaults to BASE_URL.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of strings representing the streamed output lines.\n",
    "\n",
    "    Raises:\n",
    "        requests.HTTPError: If the server response indicates failure.\n",
    "    \"\"\"\n",
    "    url = f\"{base_url}/api/pull\"\n",
    "    response = requests.post(url, json={\"name\": model}, stream=True)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise requests.HTTPError(f\"Failed to pull model '{model}': {response.text}\")\n",
    "\n",
    "    output_lines = []\n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            decoded = line.decode(\"utf-8\")\n",
    "            print(decoded)\n",
    "            output_lines.append(decoded)\n",
    "\n",
    "    return output_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90af5cb-b60c-4801-bbf3-c4c5b573f78e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Usage\n",
    "model = \"phi3:3.8b\"\n",
    "output_logs = pull_model(model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2eb35f-b9bd-4641-892c-d7ca3723994b",
   "metadata": {},
   "source": [
    "### 4. Running a Sample Query\n",
    "- Send a single chat prompt to a specified Ollama model and stream the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e47eeec-2593-46cd-9683-5c74558b8a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from typing import List\n",
    "\n",
    "def chat_once(model: str, prompt: str, base_url: str = BASE_URL) -> List[str]:\n",
    "    \"\"\"\n",
    "    Send a single chat prompt to a specified Ollama model and stream the response.\n",
    "\n",
    "    Args:\n",
    "        model (str): Name of the Ollama model to use.\n",
    "        prompt (str): User input to send to the model.\n",
    "        base_url (str, optional): Base URL of the Ollama server. Defaults to BASE_URL.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of streamed output chunks from the model.\n",
    "\n",
    "    Raises:\n",
    "        requests.HTTPError: If the server response status is not 200.\n",
    "    \"\"\"\n",
    "    url = f\"{base_url}/api/chat\"\n",
    "    response = requests.post(\n",
    "        url,\n",
    "        json={\"model\": model, \"messages\": [{\"role\": \"user\", \"content\": prompt}]},\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise requests.HTTPError(f\"Failed to chat with model '{model}': {response.text}\")\n",
    "\n",
    "    output_lines = []\n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            data = json.loads(line.decode('utf-8'))\n",
    "            if \"message\" in data:\n",
    "                content = data[\"message\"][\"content\"]\n",
    "                print(content, end=\"\", flush=True)  # Stream to console\n",
    "                output_lines.append(content)\n",
    "\n",
    "    print()  # Newline after full response\n",
    "    return output_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32a891f5-c143-49c0-9675-4fbc30a6cad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking how old I am. I need to respond in a friendly and helpful way. First, I should acknowledge their question. Then, I can mention that I'm an AI assistant and that I'm only a few years old. I should add something about my capabilities, like being available 24/7, but also mention that I can assist with various tasks. It's important to keep the tone positive and offer support. Let me make sure I'm concise and friendly.\n",
      "</think>\n",
      "\n",
      "I'm a very small AI assistant, but I'm only a few years old! I'm available 24/7 and can help with a wide range of tasks. Let me know how I can assist you! üòä\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "model=\"qwen3:0.6b\"\n",
    "prompt= \"How old are you\"\n",
    "output_logs = chat_once(model=model, prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a42109-2540-4062-9e31-0505703411b5",
   "metadata": {},
   "source": [
    "### 5. Interactive Chat with Ollama Models\n",
    "- This function enables a live, interactive conversation with a local Ollama LLM model.\n",
    "- Users can type messages in the terminal, and the model streams its responses in real time.\n",
    "- Features:\n",
    "    - Maintains conversation history between user and model.\n",
    "    - Supports multiple local models (must be pulled beforehand).\n",
    "    - Type 'exit' or 'quit' to end the session.\n",
    "    - Returns the full conversation history for further processing or logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3017d794-7f7c-4159-aeec-3779f045c58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from typing import List, Dict\n",
    "\n",
    "def ollama_chat(\n",
    "    model: str,\n",
    "    base_url: str = BASE_URL,\n",
    "    system_prompt: str = 'You are a helpful assistant. You only give a short sentence by answer.'\n",
    ") -> List [Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Start an interactive chat session with a local Ollama model via HTTP streaming.\n",
    "\n",
    "    This function streams responses from the model in real time, maintains conversation\n",
    "    history, and allows the user to exit by typing 'exit'. A system prompt can guide\n",
    "    the assistant's behavior.\n",
    "\n",
    "    Args:\n",
    "        model (str): Name of the local Ollama model to use.\n",
    "        base_url (str, optional): Base URL of the Ollama server. Defaults to BASE_URL.\n",
    "        system_prompt (str, optional): Instruction for the assistant. Defaults to a short-answer style.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, str]]: Full conversation history as a list of messages with roles ('user' or 'assistant').\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the requested model is not in the local models list.\n",
    "        requests.HTTPError: If the chat request fails.\n",
    "    \"\"\"\n",
    "    # Validate model existence\n",
    "    if model not in get_local_models():\n",
    "        raise ValueError(f\"Requested model '{model}' is not in the local list. Pull the model first!\")\n",
    "\n",
    "    # Initialize message history\n",
    "    history: List[Dict[str, str]] = []\n",
    "\n",
    "    print(\"ü§ñ Chat started ‚Äî type 'exit' to quit.\\n\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"üë§ You: \").strip()\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"üëã Goodbye!\")\n",
    "            break\n",
    "    \n",
    "        # Compose full message payload with system + history\n",
    "        request_messages = [{'role': 'system', 'content': system_prompt}] + history + [{'role': 'user', 'content': user_input}]\n",
    "    \n",
    "        # Start request\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{base_url}/api/chat\",\n",
    "                json={\"model\": model, \"messages\": request_messages},\n",
    "                stream=True\n",
    "            )\n",
    "\n",
    "            if response.status_code != 200:\n",
    "                raise requests.HTTPError(f\"Chat request failed: {response.text}\")\n",
    "\n",
    "            assistant_reply = \"\"\n",
    "            print(\"ü§ñ Ollama:\", end=\" \", flush=True)\n",
    "    \n",
    "            for line in response.iter_lines():\n",
    "                if line:\n",
    "                    data = json.loads(line.decode(\"utf-8\"))\n",
    "                    if \"message\" in data and \"content\" in data[\"message\"]:\n",
    "                        chunk = data[\"message\"][\"content\"]\n",
    "                        assistant_reply += chunk\n",
    "                        print(chunk, end='', flush=True)\n",
    "    \n",
    "            print(\"\\n\")\n",
    "    \n",
    "            # Add interaction to message history\n",
    "            history.append({'role': 'user', 'content': user_input})\n",
    "            history.append({'role': 'assistant', 'content': assistant_reply})\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(\"\\n‚ö†Ô∏è Error:\", e)\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "017e28cf-797f-45ae-9d4f-ca1e30071444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Chat started ‚Äî type 'exit' to quit.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "üë§ You:  Hello, What is HPC?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Ollama: <think>\n",
      "Okay, the user is asking what HPC is. I need to explain High-Performance Computing. Let me start by defining it clearly. High-Performance Computing refers to the use of powerful computers to solve complex problems quickly. I should mention the different types, like supercomputers, clusters, and cloud computing. Also, I should highlight its benefits like speed, efficiency, and impact on various fields. Make sure to keep it simple and concise. Let me check if I need to mention any specific applications or technologies. No, just the general benefits. Alright, time to put this into a short sentence.\n",
      "</think>\n",
      "\n",
      "High-Performance Computing (HPC) is a technology that uses powerful computers to solve complex problems faster and more efficiently.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "üë§ You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üëã Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "model = \"qwen3:0.6b\"\n",
    "history = ollama_chat(model='qwen3:0.6b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6235e0e-77fe-4e16-abc3-3f5f3c6d4bf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
