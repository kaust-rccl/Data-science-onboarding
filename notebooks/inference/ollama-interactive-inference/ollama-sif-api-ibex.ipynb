{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ef82580-be33-446a-a789-f2ce06a6989d",
   "metadata": {},
   "source": [
    "# OLLAMA - REST API Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65906f71",
   "metadata": {},
   "source": [
    "> This page was generated from [ollama-interactive-inference/ollama-sif-api-ibex.ipynb](https://github.com/kaust-rccl/Data-science-onboarding/tree/main/notebooks/inference/ollama-interactive-inference/ollama-sif-api-ibex.ipynb). You can [view or Download notebook](https://github.com/kaust-rccl/Data-science-onboarding/tree/main/notebooks/inference/ollama-interactive-inference/ollama-sif-api-ibex.ipynb). Or [view it on nbviewer](https://nbviewer.org/github/kaust-rccl/Data-science-onboarding/tree/main/notebooks/inference/ollama-interactive-inference/ollama-sif-api-ibex.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494d3541",
   "metadata": {},
   "source": [
    "## Objective\n",
    "In this notebook, we are going to use Ollama using REST API approach \n",
    "\n",
    "## Initial Setup\n",
    "If you have not completed the initial Conda environment setup and JupyterLab access steps, please refer to [OLLama on Ibex Guide - Approach-2: Notebook Workflows (Jupyter Bases).](https://docs.hpc.kaust.edu.sa/soft_env/job_schd/slurm/interactive_jobs/ollama/index.html#approach-2-notebook-workflows-jupyter-based)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82575050-725b-493d-a0f9-c7b8f37708d7",
   "metadata": {},
   "source": [
    "## Starting the Ollama Server\n",
    "Start the OLLAMA REST API server using the following bash script in a terminal:\n",
    "\n",
    "The script has the following:\n",
    "- A user editable section, where the user defines Ollama models scratch directory.\n",
    "- The allocated port is saved in a temporary ollama_port.txt file, in order to be used in the Python notebook to read the assigned port to Ollama server.\n",
    "- Cleanup section in order to stop the singularity instance when the script is terminated.\n",
    "\n",
    "### User Modification Section\n",
    "- This section of the script is reserved for user-specific setup to set the directory where the Ollama models are pulled locally.\n",
    "- In the script, you will find a clearly marked block:\n",
    "    ```bash\n",
    "    # ------------------------------------\n",
    "    # START OF USER MODIFICATION SECTION\n",
    "    # ------------------------------------\n",
    "    ```\n",
    "    \n",
    "> Note: Do not modify other parts of the script unless you are sure, as they are required for correct execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c2ecd8a-d74d-4704-a545-a0755462e7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running pre-start cleanup...\n",
      "Cleanup complete â€” ready to start new instance.\n",
      "Loading module for Singularity\n",
      "Singularity 3.9.7 modules now loaded\n",
      "OLLAMA PORT: 53639  -- Stored in ollama_port.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ollama-server-start.sh: line 9: singularity: command not found\n",
      "FATAL:   Image file already exists: \"ollama.sif\" - will not overwrite\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama server started. Logs at: /ibex/user/solimaay/scripts/jupyter/631115-ollama-sif/ibex-nb/ollama_server.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:    instance started successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['bash', 'ollama-server-start.sh'], returncode=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, subprocess\n",
    "\n",
    "script_content = \"\"\"\n",
    "#!/bin/bash\n",
    "\n",
    "# Pre-start cleanup: ensure no stale instances or files\n",
    "pre_cleanup() {\n",
    "    echo \"Running pre-start cleanup...\"\n",
    "\n",
    "    # 1. Stop any running Singularity instance with the same name\n",
    "    if singularity instance list | grep -q \"$SINGULARITY_INSTANCE_NAME\"; then\n",
    "        echo \"Stopping existing Singularity instance: $SINGULARITY_INSTANCE_NAME\"\n",
    "        singularity instance stop \"$SINGULARITY_INSTANCE_NAME\"\n",
    "    fi\n",
    "\n",
    "    # 2. Remove old temporary or state files\n",
    "    if [ -n \"$OLLAMA_PORT_TXT_FILE\" ] && [ -f \"$OLLAMA_PORT_TXT_FILE\" ]; then\n",
    "        echo \"Removing old port file: $OLLAMA_PORT_TXT_FILE\"\n",
    "        rm -f \"$OLLAMA_PORT_TXT_FILE\"\n",
    "    fi\n",
    "\n",
    "    if [ -n \"$OLLAMA_LOG_FILE\" ] && [ -f \"$OLLAMA_LOG_FILE\" ]; then\n",
    "        echo \"Removing old log file: $OLLAMA_LOG_FILE\"\n",
    "        rm -f \"$OLLAMA_LOG_FILE\"\n",
    "    fi\n",
    "\n",
    "    echo \"Cleanup complete â€” ready to start new instance.\"\n",
    "}\n",
    "\n",
    "# Cleanup process while exiting the server\n",
    "cleanup() {\n",
    "    echo \"ðŸ§¹   Cleaning up before exit...\"\n",
    "    # Put your exit commands here, e.g.:\n",
    "    rm -f $OLLAMA_PORT_TXT_FILE\n",
    "    # Remove the Singularity instance\n",
    "    singularity instance stop $SINGULARITY_INSTANCE_NAME\n",
    "}\n",
    "trap cleanup SIGINT  # Catch Ctrl+C (SIGINT) and run cleanup\n",
    "pre_cleanup\n",
    "\n",
    "# --------------------------------\n",
    "# START OF USER MODIFICATION SECTION\n",
    "# --------------------------------\n",
    "# Make target directory on /ibex/user/$USER/ollama_models_scratch to store your Ollama models\n",
    "export OLLAMA_MODELS_SCRATCH=/ibex/user/$USER/ollama_models_scratch\n",
    "# --------------------------------\n",
    "# END OF USER Editable Section\n",
    "# --------------------------------\n",
    "\n",
    "mkdir -p $OLLAMA_MODELS_SCRATCH\n",
    "\n",
    "SINGULARITY_INSTANCE_NAME='ollama'\n",
    "SINGULARITY_SIF_FILE=\"${SINGULARITY_INSTANCE_NAME}.sif\"\n",
    "OLLAMA_PORT_TXT_FILE='ollama_port.txt'\n",
    "LOG_FILE=$PWD/ollama_server.log\n",
    "\n",
    "# 2. Load Singularity module\n",
    "module load singularity\n",
    "\n",
    "# 3. Pull OLLAMA docker image\n",
    "singularity pull --name $SINGULARITY_SIF_FILE docker://ollama/ollama\n",
    "\n",
    "# 4. Change the default port for OLLAMA_HOST: (default 127.0.0.1:11434)\n",
    "export PORT=$(python -c 'import socket; s=socket.socket(); s.bind((\"\", 0)); print(s.getsockname()[1]); s.close()')\n",
    "\n",
    "# 5. Copy the assigned port, it will be required in the second part during working on the notebook.\n",
    "echo \"$PORT\" > $OLLAMA_PORT_TXT_FILE\n",
    "\n",
    "echo \"OLLAMA PORT: $PORT  -- Stored in $OLLAMA_PORT_TXT_FILE\"\n",
    "\n",
    "# 6. Define the OLLAMA Host\n",
    "export SINGULARITYENV_OLLAMA_HOST=127.0.0.1:$PORT\n",
    "\n",
    "# 7. Change the default model directory stored: \n",
    "export SINGULARITYENV_OLLAMA_MODELS=$OLLAMA_MODELS_SCRATCH\n",
    "\n",
    "# 8. Create an Instance:\n",
    "singularity instance start --nv -B \"/ibex/user:/ibex/user\" $SINGULARITY_SIF_FILE $SINGULARITY_INSTANCE_NAME\n",
    "\n",
    "# 7. Run the OLLAMA REST API server on the background\n",
    "nohup singularity exec instance://$SINGULARITY_INSTANCE_NAME bash -c \"ollama serve\" > $LOG_FILE 2>&1 &\n",
    "echo \"Ollama server started. Logs at: $LOG_FILE\"\n",
    "\"\"\"\n",
    "\n",
    "# Write script file\n",
    "script_path = \"ollama-server-start.sh\"\n",
    "with open(script_path, \"w\") as f:\n",
    "    f.write(script_content)\n",
    "os.chmod(script_path, 0o755)\n",
    "\n",
    "# Run script\n",
    "subprocess.run([\"bash\", script_path])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51368ec3-d5b9-469f-8475-c66d80ba20c3",
   "metadata": {},
   "source": [
    "## Using REST API Requests\n",
    "Follow the following Python notebook below, it contains the codes for:\n",
    "- Initialization Setup.\n",
    "- List local models.\n",
    "- Pull models.\n",
    "- Testing connection to the Ollama server.\n",
    "- Chat with the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107ad806",
   "metadata": {},
   "source": [
    "### 1. Initialization\n",
    "1. Define the base URL for the remote Ollama Server.\n",
    "2. Testing the Ollama server connectivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e5a36c6-1cd6-4c79-8cc5-274b4d51dced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://127.0.0.1:50677\n"
     ]
    }
   ],
   "source": [
    "# 1.1- Define the base URL for the remote Ollama Server\n",
    "with open(\"ollama_port.txt\") as f :\n",
    "    PORT = f.read().strip()\n",
    "    \n",
    "BASE_URL=f\"http://127.0.0.1:{PORT}\"\n",
    "print(BASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e4f918a-1880-481d-b3d3-542a9427131e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama is running! 200\n"
     ]
    }
   ],
   "source": [
    "# 1.2- Testing the Ollama server connectivity\n",
    "import requests\n",
    "\n",
    "try:\n",
    "    r = requests.get(BASE_URL)\n",
    "    print(\"Ollama is running!\", r.status_code)\n",
    "except requests.ConnectionError as e:\n",
    "    print(\"Ollama is NOT reachable:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31f5755-a214-4457-ad51-cb2879e93f97",
   "metadata": {},
   "source": [
    "### 2. Get a List of Local Models\n",
    "- Get a list of locally available Ollama models.\n",
    "- Locally available models are located under path: */ibex/user/$USER/ollama_models_scratch*\n",
    "- To change the location for pulled models, modify the variable *OLLAMA_MODELS_SCRATCH* in the script*start_ollama_server.sh*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4293ef11-0d95-43c6-b9f8-39342dcae500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gemma3:270m', 'phi3:3.8b', 'qwen3:0.6b']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a list of downloaded models\n",
    "def get_local_models(base_url: str = BASE_URL):\n",
    "    \"\"\"\n",
    "    Returns a list of locally available Ollama Models.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of model names as strings\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError: If there is a failure to connect the Ollama server.\n",
    "    \"\"\"\n",
    "    r = requests.get(f\"{base_url}/api/tags\")\n",
    "    if r.ok:\n",
    "        models = r.json().get(\"models\", [])\n",
    "        return [m[\"name\"] for m in models]\n",
    "    else:\n",
    "        raise RuntimeError(f\"Failed to list models: {r.text}\")\n",
    "\n",
    "get_local_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627a45b1-f2c1-4088-a190-a9d5b70e5630",
   "metadata": {},
   "source": [
    "### 3. Pull The Model\n",
    "- Pull a model from the Ollama server and stream the download progress.\n",
    "- Please refer to [Ollama Library](https://ollama.com/library) to check available models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20cd9a8f-ebb7-4ec3-92d3-eb516a824569",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pull the required model\n",
    "import requests\n",
    "\n",
    "def pull_model(model: str, base_url: str =BASE_URL) -> list:\n",
    "    \"\"\"\n",
    "    Pull a model from the Ollama server and stream the download progress.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the model to pull.\n",
    "        base_url (str, optional): Base URL of the Ollama server. Defaults to BASE_URL.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of strings representing the streamed output lines.\n",
    "\n",
    "    Raises:\n",
    "        requests.HTTPError: If the server response indicates failure.\n",
    "    \"\"\"\n",
    "    url = f\"{base_url}/api/pull\"\n",
    "    response = requests.post(url, json={\"name\": model}, stream=True)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise requests.HTTPError(f\"Failed to pull model '{model}': {response.text}\")\n",
    "\n",
    "    output_lines = []\n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            decoded = line.decode(\"utf-8\")\n",
    "            print(decoded)\n",
    "            output_lines.append(decoded)\n",
    "\n",
    "    return output_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90af5cb-b60c-4801-bbf3-c4c5b573f78e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Usage\n",
    "model = \"phi3:3.8b\"\n",
    "output_logs = pull_model(model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2eb35f-b9bd-4641-892c-d7ca3723994b",
   "metadata": {},
   "source": [
    "### 4. Running a Sample Query\n",
    "- Send a single chat prompt to a specified Ollama model and stream the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e47eeec-2593-46cd-9683-5c74558b8a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from typing import List\n",
    "\n",
    "def chat_once(model: str, prompt: str, base_url: str = BASE_URL) -> List[str]:\n",
    "    \"\"\"\n",
    "    Send a single chat prompt to a specified Ollama model and stream the response.\n",
    "\n",
    "    Args:\n",
    "        model (str): Name of the Ollama model to use.\n",
    "        prompt (str): User input to send to the model.\n",
    "        base_url (str, optional): Base URL of the Ollama server. Defaults to BASE_URL.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of streamed output chunks from the model.\n",
    "\n",
    "    Raises:\n",
    "        requests.HTTPError: If the server response status is not 200.\n",
    "    \"\"\"\n",
    "    url = f\"{base_url}/api/chat\"\n",
    "    response = requests.post(\n",
    "        url,\n",
    "        json={\"model\": model, \"messages\": [{\"role\": \"user\", \"content\": prompt}]},\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise requests.HTTPError(f\"Failed to chat with model '{model}': {response.text}\")\n",
    "\n",
    "    output_lines = []\n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            data = json.loads(line.decode('utf-8'))\n",
    "            if \"message\" in data:\n",
    "                content = data[\"message\"][\"content\"]\n",
    "                print(content, end=\"\", flush=True)  # Stream to console\n",
    "                output_lines.append(content)\n",
    "\n",
    "    print()  # Newline after full response\n",
    "    return output_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32a891f5-c143-49c0-9675-4fbc30a6cad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have a physical age, but I can help you with a wide range of tasks, whether you need assistance with writing, math, or anything else. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "model=\"qwen3:0.6b\"\n",
    "prompt= \"How old are you\"\n",
    "output_logs = chat_once(model=model, prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a42109-2540-4062-9e31-0505703411b5",
   "metadata": {},
   "source": [
    "### 5. Interactive Chat with Ollama Models\n",
    "- This function enables a live, interactive conversation with a local Ollama LLM model.\n",
    "- Users can type messages in the terminal, and the model streams its responses in real time.\n",
    "- Features:\n",
    "    - Maintains conversation history between user and model.\n",
    "    - Supports multiple local models (must be pulled beforehand).\n",
    "    - Type 'exit' or 'quit' to end the session.\n",
    "    - Returns the full conversation history for further processing or logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3017d794-7f7c-4159-aeec-3779f045c58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from typing import List, Dict\n",
    "\n",
    "def ollama_chat(\n",
    "    model: str,\n",
    "    base_url: str = BASE_URL,\n",
    "    system_prompt: str = 'You are a helpful assistant. You only give a short sentence by answer.'\n",
    ") -> List [Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Start an interactive chat session with a local Ollama model via HTTP streaming.\n",
    "\n",
    "    This function streams responses from the model in real time, maintains conversation\n",
    "    history, and allows the user to exit by typing 'exit'. A system prompt can guide\n",
    "    the assistant's behavior.\n",
    "\n",
    "    Args:\n",
    "        model (str): Name of the local Ollama model to use.\n",
    "        base_url (str, optional): Base URL of the Ollama server. Defaults to BASE_URL.\n",
    "        system_prompt (str, optional): Instruction for the assistant. Defaults to a short-answer style.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, str]]: Full conversation history as a list of messages with roles ('user' or 'assistant').\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the requested model is not in the local models list.\n",
    "        requests.HTTPError: If the chat request fails.\n",
    "    \"\"\"\n",
    "    # Validate model existence\n",
    "    if model not in get_local_models():\n",
    "        raise ValueError(f\"Requested model '{model}' is not in the local list. Pull the model first!\")\n",
    "\n",
    "    # Initialize message history\n",
    "    history: List[Dict[str, str]] = []\n",
    "\n",
    "    print(\"ðŸ¤– Chat started â€” type 'exit' to quit.\\n\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"ðŸ‘¤ You: \").strip()\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"ðŸ‘‹ Goodbye!\")\n",
    "            break\n",
    "    \n",
    "        # Compose full message payload with system + history\n",
    "        request_messages = [{'role': 'system', 'content': system_prompt}] + history + [{'role': 'user', 'content': user_input}]\n",
    "    \n",
    "        # Start request\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{base_url}/api/chat\",\n",
    "                json={\"model\": model, \"messages\": request_messages},\n",
    "                stream=True\n",
    "            )\n",
    "\n",
    "            if response.status_code != 200:\n",
    "                raise requests.HTTPError(f\"Chat request failed: {response.text}\")\n",
    "\n",
    "            assistant_reply = \"\"\n",
    "            print(\"ðŸ¤– Ollama:\", end=\" \", flush=True)\n",
    "    \n",
    "            for line in response.iter_lines():\n",
    "                if line:\n",
    "                    data = json.loads(line.decode(\"utf-8\"))\n",
    "                    if \"message\" in data and \"content\" in data[\"message\"]:\n",
    "                        chunk = data[\"message\"][\"content\"]\n",
    "                        assistant_reply += chunk\n",
    "                        print(chunk, end='', flush=True)\n",
    "    \n",
    "            print(\"\\n\")\n",
    "    \n",
    "            # Add interaction to message history\n",
    "            history.append({'role': 'user', 'content': user_input})\n",
    "            history.append({'role': 'assistant', 'content': assistant_reply})\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(\"\\nâš ï¸ Error:\", e)\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "017e28cf-797f-45ae-9d4f-ca1e30071444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Chat started â€” type 'exit' to quit.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ðŸ‘¤ You:  Hello, How is the weather today?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Ollama: The weather is cloudy with a light breeze.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ðŸ‘¤ You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘‹ Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "model = \"qwen3:0.6b\"\n",
    "history = ollama_chat(model='qwen3:0.6b')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169ae88e-9e8e-44f4-bdad-13c715b5fc56",
   "metadata": {},
   "source": [
    "## Stop the Ollama Server\n",
    "Make sure to stop the Ollama server by terminating the Singularity container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40d623cd-338a-4904-bd7c-1d816cfecf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "def stop_singularity_instance(instance_name=\"ollama\", log_file=None, port_file=None):\n",
    "    \"\"\"\n",
    "    Gracefully stop a running Singularity instance by name, \n",
    "    and optionally remove associated log or port files.\n",
    "    \"\"\"\n",
    "    print(f\"Checking for Singularity instance: {instance_name}\")\n",
    "\n",
    "    # 1. Check if instance is running\n",
    "    try:\n",
    "\n",
    "        result = subprocess.run(\n",
    "            'bash -lc \"module load singularity 2>/dev/null || true; singularity instance list\"',\n",
    "            shell=True,\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "\n",
    "        if instance_name not in result.stdout:\n",
    "            print(f\"No running instance named '{instance_name}' found.\")\n",
    "        else:\n",
    "            print(f\"Instance '{instance_name}' is running. Attempting to stop it...\")\n",
    "            stop_result = subprocess.run(\n",
    "                f'bash -lc \"module load singularity 2>/dev/null || true; singularity instance stop {instance_name}\"',\n",
    "                shell=True,\n",
    "                capture_output=True,\n",
    "                text=True\n",
    "            )\n",
    "            if stop_result.returncode == 0:\n",
    "                print(f\"Singularity instance '{instance_name}' stopped successfully.\")\n",
    "            else:\n",
    "                print(f\"Warning: Failed to stop instance '{instance_name}'.\")\n",
    "                print(stop_result.stderr)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"Singularity command not found. Ensure it's installed and in PATH.\")\n",
    "        return\n",
    "\n",
    "    # 2. Optional cleanup for files\n",
    "    if port_file and os.path.exists(port_file):\n",
    "        os.remove(port_file)\n",
    "        print(f\"Removed port file: {port_file}\")\n",
    "\n",
    "    if log_file and os.path.exists(log_file):\n",
    "        os.remove(log_file)\n",
    "        print(f\"Removed log file: {log_file}\")\n",
    "\n",
    "    print(\"Cleanup complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "784a11de-b7fc-4c9f-915d-981f256d66b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for Singularity instance: ollama\n",
      "Instance 'ollama' is running. Attempting to stop it...\n",
      "Singularity instance 'ollama' stopped successfully.\n",
      "Cleanup complete.\n"
     ]
    }
   ],
   "source": [
    "stop_singularity_instance(\n",
    "    instance_name=\"ollama\",\n",
    "    log_file=os.path.expandvars(\"$PWD/ollama_server.log\"),\n",
    "    port_file=os.path.expandvars(\"$PWD/ollama_port.txt\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586a6534-8208-42a7-b35a-90723c0ec892",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
