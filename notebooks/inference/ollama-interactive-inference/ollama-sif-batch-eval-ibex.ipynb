{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0ea5656",
   "metadata": {},
   "source": [
    "# Ollama Batch Evaluation Guide (LLM-as-a-Judge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9822d045",
   "metadata": {},
   "source": [
    "> This page was generated from [ollama-interactive-inference/ollama-sif-batch-eval-ibex.ipynb](https://github.com/kaust-rccl/Data-science-onboarding/tree/main/notebooks/inference/ollama-interactive-inference/ollama-sif-batch-eval-ibex.ipynb). You can [view or download notebook](https://github.com/kaust-rccl/Data-science-onboarding/tree/main/notebooks/inference/ollama-interactive-inference/ollama-sif-batch-eval-ibex.ipynb). Or [view it on nbviewer](https://nbviewer.org/github/kaust-rccl/Data-science-onboarding/tree/main/notebooks/inference/ollama-interactive-inference/ollama-sif-batch-eval-ibex.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db18baaa",
   "metadata": {},
   "source": [
    "## Objective\n",
    "This guide helps you evaluate multiple model responses automatically using Ollamaâ€™s batch evaluation feature. Instead of manually scoring outputs, an LLM acts as a judge, comparing predictions against reference answers or quality criteria you define.\n",
    "\n",
    "## Initial Setup\n",
    "If you haven't installed conda yet, please follow [`How to Setup Conda on Ibex Guide`](https://docs.hpc.kaust.edu.sa/soft_env/prog_env/python_package_management/conda/ibex.html) to get started.\n",
    "\n",
    "\n",
    "After conda has been installed, save the following environment yaml file on Ibex under the name ``ollama_env.yaml``\n",
    "\n",
    "```yml\n",
    "name: ollama_env\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - _libgcc_mutex=0.1\n",
    "  - _openmp_mutex=4.5\n",
    "  - _python_abi3_support=1.0\n",
    "  - anyio=4.11.0\n",
    "  - argon2-cffi=25.1.0\n",
    "  - argon2-cffi-bindings=25.1.0\n",
    "  - arrow=1.4.0\n",
    "  - asttokens=3.0.0\n",
    "  - async-lru=2.0.5\n",
    "  - attrs=25.4.0\n",
    "  - babel=2.17.0\n",
    "  - beautifulsoup4=4.14.2\n",
    "  - bleach=6.2.0\n",
    "  - bleach-with-css=6.2.0\n",
    "  - brotli-python=1.1.0\n",
    "  - bzip2=1.0.8\n",
    "  - ca-certificates=2025.10.5\n",
    "  - cached-property=1.5.2\n",
    "  - cached_property=1.5.2\n",
    "  - certifi=2025.10.5\n",
    "  - cffi=2.0.0\n",
    "  - charset-normalizer=3.4.4\n",
    "  - comm=0.2.3\n",
    "  - cpython=3.14.0\n",
    "  - debugpy=1.8.17\n",
    "  - decorator=5.2.1\n",
    "  - defusedxml=0.7.1\n",
    "  - exceptiongroup=1.3.0\n",
    "  - executing=2.2.1\n",
    "  - fqdn=1.5.1\n",
    "  - h11=0.16.0\n",
    "  - h2=4.3.0\n",
    "  - hpack=4.1.0\n",
    "  - httpcore=1.0.9\n",
    "  - httpx=0.28.1\n",
    "  - hyperframe=6.1.0\n",
    "  - idna=3.11\n",
    "  - importlib-metadata=8.7.0\n",
    "  - ipykernel=7.0.1\n",
    "  - ipython=9.6.0\n",
    "  - ipython_pygments_lexers=1.1.1\n",
    "  - isoduration=20.11.0\n",
    "  - jedi=0.19.2\n",
    "  - jinja2=3.1.6\n",
    "  - json5=0.12.1\n",
    "  - jsonpointer=3.0.0\n",
    "  - jsonschema=4.25.1\n",
    "  - jsonschema-specifications=2025.9.1\n",
    "  - jsonschema-with-format-nongpl=4.25.1\n",
    "  - jupyter-lsp=2.3.0\n",
    "  - jupyter_client=8.6.3\n",
    "  - jupyter_core=5.9.1\n",
    "  - jupyter_events=0.12.0\n",
    "  - jupyter_server=2.17.0\n",
    "  - jupyter_server_terminals=0.5.3\n",
    "  - jupyterlab=4.4.9\n",
    "  - jupyterlab_pygments=0.3.0\n",
    "  - jupyterlab_server=2.27.3\n",
    "  - keyutils=1.6.3\n",
    "  - krb5=1.21.3\n",
    "  - lark=1.3.0\n",
    "  - ld_impl_linux-64=2.44\n",
    "  - libedit=3.1.20250104\n",
    "  - libexpat=2.7.1\n",
    "  - libffi=3.4.6\n",
    "  - libgcc=15.2.0\n",
    "  - libgcc-ng=15.2.0\n",
    "  - libgomp=15.2.0\n",
    "  - liblzma=5.8.1\n",
    "  - libmpdec=4.0.0\n",
    "  - libsodium=1.0.20\n",
    "  - libsqlite=3.50.4\n",
    "  - libstdcxx=15.2.0\n",
    "  - libstdcxx-ng=15.2.0\n",
    "  - libuuid=2.41.2\n",
    "  - libzlib=1.3.1\n",
    "  - markupsafe=3.0.3\n",
    "  - matplotlib-inline=0.1.7\n",
    "  - mistune=3.1.4\n",
    "  - nbclient=0.10.2\n",
    "  - nbconvert-core=7.16.6\n",
    "  - nbformat=5.10.4\n",
    "  - ncurses=6.5\n",
    "  - nest-asyncio=1.6.0\n",
    "  - notebook-shim=0.2.4\n",
    "  - openssl=3.5.4\n",
    "  - overrides=7.7.0\n",
    "  - packaging=25.0\n",
    "  - pandocfilters=1.5.0\n",
    "  - parso=0.8.5\n",
    "  - pexpect=4.9.0\n",
    "  - pickleshare=0.7.5\n",
    "  - pip=25.2\n",
    "  - platformdirs=4.5.0\n",
    "  - prometheus_client=0.23.1\n",
    "  - prompt-toolkit=3.0.52\n",
    "  - psutil=7.1.0\n",
    "  - ptyprocess=0.7.0\n",
    "  - pure_eval=0.2.3\n",
    "  - pycparser=2.22\n",
    "  - pygments=2.19.2\n",
    "  - pysocks=1.7.1\n",
    "  - python=3.14.0\n",
    "  - python-dateutil=2.9.0.post0\n",
    "  - python-fastjsonschema=2.21.2\n",
    "  - python-gil=3.14.0\n",
    "  - python-json-logger=2.0.7\n",
    "  - python-tzdata=2025.2\n",
    "  - python_abi=3.14\n",
    "  - pytz=2025.2\n",
    "  - pyyaml=6.0.3\n",
    "  - pyzmq=27.1.0\n",
    "  - readline=8.2\n",
    "  - referencing=0.37.0\n",
    "  - requests=2.32.5\n",
    "  - rfc3339-validator=0.1.4\n",
    "  - rfc3986-validator=0.1.1\n",
    "  - rfc3987-syntax=1.1.0\n",
    "  - rpds-py=0.27.1\n",
    "  - send2trash=1.8.3\n",
    "  - setuptools=80.9.0\n",
    "  - six=1.17.0\n",
    "  - sniffio=1.3.1\n",
    "  - soupsieve=2.8\n",
    "  - stack_data=0.6.3\n",
    "  - terminado=0.18.1\n",
    "  - tinycss2=1.4.0\n",
    "  - tk=8.6.13\n",
    "  - tomli=2.3.0\n",
    "  - tornado=6.5.2\n",
    "  - traitlets=5.14.3\n",
    "  - typing-extensions=4.15.0\n",
    "  - typing_extensions=4.15.0\n",
    "  - typing_utils=0.1.0\n",
    "  - tzdata=2025b\n",
    "  - uri-template=1.3.0\n",
    "  - urllib3=2.5.0\n",
    "  - wcwidth=0.2.14\n",
    "  - webcolors=24.11.1\n",
    "  - webencodings=0.5.1\n",
    "  - websocket-client=1.9.0\n",
    "  - yaml=0.2.5\n",
    "  - zeromq=4.3.5\n",
    "  - zipp=3.23.0\n",
    "  - zstandard=0.25.0\n",
    "  - zstd=1.5.7\n",
    "  - pip:\n",
    "      - annotated-types==0.7.0\n",
    "      - ollama==0.6.0\n",
    "      - pydantic==2.12.3\n",
    "      - pydantic-core==2.41.4\n",
    "      - typing-inspection==0.4.2\n",
    "```\n",
    "\n",
    "Run the following command to build the conda environment:\n",
    "```bash\n",
    "conda env create -f ollama_env.yaml\n",
    "```\n",
    "\n",
    "## Starting JupyterLab\n",
    "Follow [`Guide: Using Jupyter on Ibex`](https://docs.hpc.kaust.edu.sa/soft_env/job_schd/slurm/interactive_jobs/jupyter.html#job-on-ibex) to start JupyterLab on a an Ibex GPU node using your conda environment instead of *'machine_learning'* module.\n",
    "\n",
    "By making the following changes to the Jupyter launch script:\n",
    "```bash\n",
    "#module load machine_learning/2024.01\n",
    "conda activate ollama_env\n",
    "```\n",
    "\n",
    "## Starting The Ollama Server\n",
    "Start the OLLAMA REST API server using the following bash script in a terminal:\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "# Cleanup process while exiting the server\n",
    "cleanup() {\n",
    "    echo \"ðŸ§¹   Cleaning up before exit...\"\n",
    "    # Put your exit commands here, e.g.:\n",
    "    rm -f $OLLAMA_PORT_TXT_FILE\n",
    "    # Remove the Singularity instance\n",
    "    singularity instance stop $SINGULARITY_INSTANCE_NAME\n",
    "}\n",
    "trap cleanup SIGINT  # Catch Ctrl+C (SIGINT) and run cleanup\n",
    "\n",
    "# User Editable Section\n",
    "# 1. Make target directory on /ibex/user/$USER/ollama_models_scratch to store your Ollama models\n",
    "export OLLAMA_MODELS_SCRATCH=/ibex/user/$USER/ollama_models_scratch\n",
    "mkdir -p $OLLAMA_MODELS_SCRATCH\n",
    "# End of User Editable Section\n",
    "\n",
    "SINGULARITY_INSTANCE_NAME=\"ollama\"\n",
    "OLLAMA_PORT_TXT_FILE='ollama_port.txt'\n",
    "\n",
    "# 2. Load Singularity module\n",
    "module load singularity\n",
    "\n",
    "# 3. Pull OLLAMA docker image\n",
    "singularity pull docker://ollama/ollama\n",
    "\n",
    "# 4. Change the default port for OLLAMA_HOST: (default 127.0.0.1:11434)\n",
    "export PORT=$(python -c 'import socket; s=socket.socket(); s.bind((\"\", 0)); print(s.getsockname()[1]); s.close()')\n",
    "\n",
    "# 5. Copy the assigned port, it will be required in the second part during working on the notebook.\n",
    "echo \"$PORT\" > $OLLAMA_PORT_TXT_FILE\n",
    "\n",
    "echo \"OLLAMA PORT: $PORT  -- Stored in $OLLAMA_PORT_TXT_FILE\"\n",
    "\n",
    "# 6. Define the OLLAMA Host\n",
    "export SINGULARITYENV_OLLAMA_HOST=127.0.0.1:$PORT\n",
    "\n",
    "# 7. Change the default model directory stored: (default ~/.ollama/models/manifests/registry.ollama.ai/library)\n",
    "export SINGULARITYENV_OLLAMA_MODELS=$OLLAMA_MODELS_SCRATCH\n",
    "\n",
    "# 8. Create an Instance:\n",
    "singularity instance start --nv -B \"/ibex/user:/ibex/user\" ollama_latest.sif $SINGULARITY_INSTANCE_NAME\n",
    "\n",
    "# 7. Run the OLLAMA REST API server on the background\n",
    "singularity exec instance://$SINGULARITY_INSTANCE_NAME bash -c \"ollama serve\"\n",
    "```\n",
    "\n",
    "> Note: Save the above script in a file called start_ollama_server.sh\n",
    "\n",
    "```bash\n",
    "# Run the script to start the Ollama server.\n",
    "bash start_ollama_server.sh\n",
    "```\n",
    "\n",
    "The script has the following:\n",
    "- A user editable section, where the user defines Ollama models scratch directory.\n",
    "- The allocated port is saved in a temporary ollama_port.txt file, in order to be used in the Python notebook to read the assigned port to Ollama server.\n",
    "- Cleanup section in order to stop the singularity instance when the script is terminated with CTRL+C.\n",
    "\n",
    "## Using Ollama Packages Requests\n",
    "Follow the following Python notebook below, it contains the codes for:\n",
    "- Initialization Setup.\n",
    "- List local models.\n",
    "- Pull models.\n",
    "- Testing connection to the Ollama server.\n",
    "- Chat with the models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9f194e-c96a-435f-8008-99b63126c62f",
   "metadata": {},
   "source": [
    "### 1. Initialization\n",
    "1. Define the base URL for the remote Ollama Server.\n",
    "2. Testing the Ollama server connectivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "965e31e2-2c23-48ce-bad0-d73d19e0383f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from ollama import AsyncClient\n",
    "from typing import List, Dict\n",
    "\n",
    "MAX_CONCURRENT = 2  # limit to avoid GPU overload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f97d9b1-88ba-4e32-8613-127e9d292634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://127.0.0.1:60159\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "with open(\"ollama_port.txt\") as f :\n",
    "    PORT = f.read().strip()\n",
    "    \n",
    "BASE_URL=f\"http://127.0.0.1:{PORT}\"\n",
    "print(BASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb259eaa-388a-4157-8f70-69ffdd80a931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama is running! 200\n"
     ]
    }
   ],
   "source": [
    "# Testing the server connectivity\n",
    "import requests\n",
    "\n",
    "try:\n",
    "    r = requests.get(BASE_URL)\n",
    "    print(\"Ollama is running!\", r.status_code)\n",
    "except requests.ConnectionError as e:\n",
    "    print(\"Ollama is NOT reachable:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d42bfc3-bd60-4c96-984d-54bf4112e56c",
   "metadata": {},
   "source": [
    "### 2. Get a List of Local Models\n",
    "- Get a list of locally available Ollama models.\n",
    "- Locally available models are located under path: */ibex/user/$USER/ollama_models_scratch*\n",
    "- To change the location for pulled models, modify the variable *OLLAMA_MODELS_SCRATCH* in the script*start_ollama_server.sh*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdbdc6e8-3d93-4063-b186-54a7afba8f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Client\n",
    "client = Client(\n",
    "  host=BASE_URL,\n",
    ")\n",
    "\n",
    "def get_local_models():\n",
    "    \"\"\"\n",
    "    Returns a list of locally available Ollama Models.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of model names as strings\n",
    "    \"\"\"\n",
    "    models = [model['model'] for model in client.list()['models']]\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a877e166-05a6-43fb-aed5-f817c1cd7c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gemma3:270m', 'qwen3:0.6b']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Usage\n",
    "get_local_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24aea834-7c62-47c3-9381-b31d1e281367",
   "metadata": {},
   "source": [
    "### 3. Pull The Model\n",
    "- To pull a specific model, use *pull* method.\n",
    "- Please refer to [Ollama Library](https://ollama.com/library) to check available models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "49d338e9-756a-4a79-b578-12201bc3c397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProgressResponse(status='success', completed=None, total=None, digest=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pull the required models\n",
    "client.pull(\"qwen3:0.6b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5def6220-da2c-4ae7-95fb-ecc8bcd71878",
   "metadata": {},
   "source": [
    "### 4. Running Batch Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c568725e-4b07-42cc-b625-9a63e9a95cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def ensure_model_exists(client: AsyncClient, model: str):\n",
    "    \"\"\"\n",
    "    Ensure that a specified Ollama model is available locally.  \n",
    "    If the model is not installed, it will be pulled from the server.\n",
    "\n",
    "    Args:\n",
    "        client (AsyncClient): An instance of the AsyncClient connected to the Ollama server.\n",
    "        model (str): Name of the model to check and pull if necessary.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If pulling or checking the model fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if the model is already available\n",
    "        await client.show(model)\n",
    "        print(f\"Model {model} already available locally.\")\n",
    "    except Exception:\n",
    "        # Pull the model if it does not exist\n",
    "        print(f\"Pulling model {model}...\")\n",
    "        async for progress in await client.pull(model, stream=True):\n",
    "            status = progress.get(\"status\", \"\")\n",
    "            if \"completed\" in status.lower():\n",
    "                print(f\"Pulled {model} successfully.\")\n",
    "        print(f\"Model {model} is now ready for use.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b004284-a295-4fb3-b8fa-4c90b2036ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def query_model_async(client: AsyncClient, model: str, prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Send a single prompt to a specified Ollama model asynchronously and return the full response.\n",
    "\n",
    "    Args:\n",
    "        client (AsyncClient): An instance of AsyncClient connected to the Ollama server.\n",
    "        model (str): Name of the Ollama model to query.\n",
    "        prompt (str): The user input to send to the model.\n",
    "\n",
    "    Returns:\n",
    "        str: The complete response text from the model.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If the chat request fails.\n",
    "    \"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = \"\"\n",
    "    \n",
    "    async for chunk in await client.chat(model=model, messages=messages, stream=True):\n",
    "        if chunk.get(\"message\") and \"content\" in chunk[\"message\"]:\n",
    "            response += chunk[\"message\"][\"content\"]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f0128cd-ab09-4a1c-9346-1590f0649863",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_batch(models: List[str], prompt: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Run multiple model inferences concurrently while limiting active requests.\n",
    "\n",
    "    This function uses asynchronous concurrency control to efficiently query \n",
    "    multiple models in parallel, ensuring that no more than `max_concurrent` \n",
    "    requests are active at a time. Each model is checked for availability before \n",
    "    being queried, and missing models are automatically pulled.\n",
    "\n",
    "    Args:\n",
    "        models (List[str]): A list of model names to query.\n",
    "        prompt (str): The user input or question to be sent to each model.\n",
    "        base_url (str, optional): The base URL of the Ollama API endpoint.\n",
    "        max_concurrent (int, optional): The maximum number of models to run concurrently.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, str]: A dictionary mapping model names to their response text.\n",
    "    \"\"\"\n",
    "    client = AsyncClient(host=BASE_URL)\n",
    "    semaphore = asyncio.Semaphore(MAX_CONCURRENT)\n",
    "\n",
    "    async def safe_query(model):\n",
    "        async with semaphore:\n",
    "            # Ensure model is available before querying\n",
    "            await ensure_model_exists(client, model)\n",
    "            print(f\"Running {model}...\")\n",
    "            result = await query_model_async(client, model, prompt)\n",
    "            print(f\"Done: {model}\")\n",
    "            return model, result\n",
    "\n",
    "    results = await asyncio.gather(*(safe_query(m) for m in models))\n",
    "    return dict(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0eaa0a66-0dc1-4080-93d7-67e97678b2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def judge_model_responses(\n",
    "    client: AsyncClient,\n",
    "    judge_model: str,\n",
    "    responses: Dict[str, str],\n",
    "    criteria: str\n",
    ") -> Dict[str, Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Use an LLM to evaluate the outputs of other models according to a given criteria.\n",
    "\n",
    "    Args:\n",
    "        client (AsyncClient): An instance of AsyncClient connected to the Ollama server.\n",
    "        responses (Dict[str, str]): A dictionary mapping model names to their outputs.\n",
    "        criteria (str): Evaluation criteria to guide the judging process.\n",
    "        judge_model (str, optional): The model used as the judge. Defaults to \"llama3\".\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Dict[str, str]]: A dictionary mapping each evaluated model to its judgment,\n",
    "        containing keys like \"evaluation\" with the judge's reasoning and score.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If model evaluation fails or judge model cannot be ensured.\n",
    "    \"\"\"\n",
    "    judged = {}\n",
    "    \n",
    "    # Ensure the judge model exists locally\n",
    "    await ensure_model_exists(client, judge_model)\n",
    "\n",
    "    for model, answer in responses.items():\n",
    "        judge_prompt = f\"\"\"\n",
    "You are an impartial judge. Evaluate the following answer according to {criteria}.\n",
    "\n",
    "Answer:\n",
    "{answer}\n",
    "\n",
    "Give:\n",
    "- Reasoning\n",
    "- Score from 1 to 10\n",
    "\"\"\"\n",
    "        eval_resp = await query_model_async(client, judge_model, judge_prompt)\n",
    "        judged[model] = {\"evaluation\": eval_resp}\n",
    "        \n",
    "    return judged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "892ba43c-7632-4847-862a-32e687e39e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    # Define List of Models [model_1, model_2, ...]\n",
    "    models = ['gemma3:270m']\n",
    "    # Define the Judge LLM model\n",
    "    judge_model = \"qwen3:0.6b\"\n",
    "    # Define the prompt\n",
    "    prompt = \"What is 1+1 equal?\"\n",
    "    client = AsyncClient(host=BASE_URL)\n",
    "\n",
    "    # 1. Run models (with auto-pull if missing)\n",
    "    responses = await run_batch(models, prompt)\n",
    "\n",
    "    # 2. Judge responses\n",
    "    evaluations = await judge_model_responses(client, judge_model, responses, \"clarity, correctness, and conciseness\")\n",
    "\n",
    "    # 3. Display\n",
    "    for m in models:\n",
    "        print(f\"\\n--- {m} ---\")\n",
    "        print(\"Response:\\n\", responses[m])\n",
    "        print(\"Evaluation:\\n\", evaluations[m][\"evaluation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "689e1ab9-712e-4dc4-9850-94e089c61675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model gemma3:270m already available locally.\n",
      "Running gemma3:270m...\n",
      "Done: gemma3:270m\n",
      "Model qwen3:0.6b already available locally.\n",
      "\n",
      "--- gemma3:270m ---\n",
      "Response:\n",
      " 1 + 1 = 2\n",
      "\n",
      "Evaluation:\n",
      " <think>\n",
      "Okay, let's see what the user is asking for. They want me to evaluate the answer \"1 + 1 = 2\" according to clarity, correctness, and conciseness. Then they also want a score from 1 to 10. \n",
      "\n",
      "First, I need to check if the answer is correct. The expression 1 + 1 equals 2 is correct, so that's a solid correctness. But wait, the user mentioned \"Evaluate the following answer according to clarity, correctness, and conciseness.\" So maybe they want me to look at how clearly it's presented. The answer is very straightforward, so clarity is good. \n",
      "\n",
      "Now, the reasoning part. The user provided a \"Reasoning\" section, but in the given answer, there's only the equation. Maybe there's a mistake here? Wait, the user's example shows that they have a \"Reasoning\" section. But in the answer, there's only the equation. So perhaps the user wants me to evaluate that part as well. But the answer given is just the equation. Maybe there's confusion here. Let me check again. The user says \"Answer: 1 + 1 = 2. Give: - Reasoning...\" but in the answer provided, there's only the equation. So maybe there's a formatting issue where the user intended to put the reasoning in a separate box, but in this case, they just included the equation. \n",
      "\n",
      "If the user is asking for a reasoning explanation, but the answer itself is just an equation, then maybe the reasoning section should be part of the answer. However, in the given answer, there's only the equation. So perhaps the user made an error in formatting. In that case, the reasoning part should be included, but since the answer is just the equation, maybe the reasoning is left blank. But the user included a \"Reasoning\" part, so maybe they wanted me to evaluate that as well. Wait, the original question is in Chinese, and the user is using the answer as an example. Let me rephrase. The user is asking to evaluate the answer \"1 + 1 = 2\" based on clarity, correctness, and conciseness, and then to give a score. The answer provided by the user includes the equation, but the reasoning part is missing. So maybe the user intended to include the reasoning, but due to formatting, it's not there. In that case, the reasoning part is incomplete, which might lower the score. \n",
      "\n",
      "So, clarity is there, correctness is correct, conciseness is good. The reasoning part is missing, so that's a problem. The score could be 7 or 8. Alternatively, if the reasoning is included, it might be higher. But since it's missing, the score would be lower. \n",
      "\n",
      "Wait, but the user included a \"Reasoning\" section in their example. Maybe they want me to evaluate that as well. However, in the answer given, there's only the equation. So perhaps there's confusion here. To clarify, the user's example shows that they expect the reasoning section to be part of the answer. But in this case, the answer only has the equation. Therefore, the reasoning is incomplete. \n",
      "\n",
      "So, putting it all together: the answer is correct, but the reasoning is missing. So the score could be 7-8. Clarity is present, correctness is correct, conciseness is good. Reasoning is missing, so that's a point to lower the score.\n",
      "</think>\n",
      "\n",
      "**Evaluation of Answer:**\n",
      "\n",
      "- **Clarity:** The answer is clear and straightforward. It correctly states the mathematical relationship between 1 and 2.  \n",
      "- **Correctness:** The equation is mathematically correct.  \n",
      "- **Conciseness:** The answer is concise and directly addresses the question.  \n",
      "\n",
      "**Reasoning Score:** 7 (out of 10)  \n",
      "**Explanation:** The answer is correct but lacks a complete reasoning section. While it fulfills the requirement of clarity and correctness, the missing reasoning component reduces the score.  \n",
      "\n",
      "**Final Score:** 7\n"
     ]
    }
   ],
   "source": [
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379b154e-86ce-44dc-b0a8-45adb4b117cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
