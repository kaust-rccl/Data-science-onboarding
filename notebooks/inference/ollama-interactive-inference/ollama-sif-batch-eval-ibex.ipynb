{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0ea5656",
   "metadata": {},
   "source": [
    "# Ollama Batch Evaluation Guide (LLM-as-a-Judge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9822d045",
   "metadata": {},
   "source": [
    "> This page was generated from [ollama-interactive-inference/ollama-sif-batch-eval-ibex.ipynb](https://github.com/kaust-rccl/Data-science-onboarding/tree/main/notebooks/inference/ollama-interactive-inference/ollama-sif-batch-eval-ibex.ipynb). You can [view or download notebook](https://github.com/kaust-rccl/Data-science-onboarding/tree/main/notebooks/inference/ollama-interactive-inference/ollama-sif-batch-eval-ibex.ipynb). Or [view it on nbviewer](https://nbviewer.org/github/kaust-rccl/Data-science-onboarding/tree/main/notebooks/inference/ollama-interactive-inference/ollama-sif-batch-eval-ibex.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db18baaa",
   "metadata": {},
   "source": [
    "## Objective\n",
    "This guide helps you evaluate multiple model responses automatically using Ollamaâ€™s batch evaluation feature. Instead of manually scoring outputs, an LLM acts as a judge, comparing predictions against reference answers or quality criteria you define.\n",
    "\n",
    "## Initial Setup\n",
    "If you haven't installed conda yet, please follow [`How to Setup Conda on Ibex Guide`](https://docs.hpc.kaust.edu.sa/soft_env/prog_env/python_package_management/conda/ibex.html) to get started.\n",
    "\n",
    "\n",
    "After conda has been installed, save the following environment yaml file on Ibex under the name ``ollama_env.yaml``\n",
    "\n",
    "```yml\n",
    "name: ollama_env\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - _libgcc_mutex=0.1\n",
    "  - _openmp_mutex=4.5\n",
    "  - _python_abi3_support=1.0\n",
    "  - anyio=4.11.0\n",
    "  - argon2-cffi=25.1.0\n",
    "  - argon2-cffi-bindings=25.1.0\n",
    "  - arrow=1.4.0\n",
    "  - asttokens=3.0.0\n",
    "  - async-lru=2.0.5\n",
    "  - attrs=25.4.0\n",
    "  - babel=2.17.0\n",
    "  - beautifulsoup4=4.14.2\n",
    "  - bleach=6.2.0\n",
    "  - bleach-with-css=6.2.0\n",
    "  - brotli-python=1.1.0\n",
    "  - bzip2=1.0.8\n",
    "  - ca-certificates=2025.10.5\n",
    "  - cached-property=1.5.2\n",
    "  - cached_property=1.5.2\n",
    "  - certifi=2025.10.5\n",
    "  - cffi=2.0.0\n",
    "  - charset-normalizer=3.4.4\n",
    "  - comm=0.2.3\n",
    "  - cpython=3.14.0\n",
    "  - debugpy=1.8.17\n",
    "  - decorator=5.2.1\n",
    "  - defusedxml=0.7.1\n",
    "  - exceptiongroup=1.3.0\n",
    "  - executing=2.2.1\n",
    "  - fqdn=1.5.1\n",
    "  - h11=0.16.0\n",
    "  - h2=4.3.0\n",
    "  - hpack=4.1.0\n",
    "  - httpcore=1.0.9\n",
    "  - httpx=0.28.1\n",
    "  - hyperframe=6.1.0\n",
    "  - idna=3.11\n",
    "  - importlib-metadata=8.7.0\n",
    "  - ipykernel=7.0.1\n",
    "  - ipython=9.6.0\n",
    "  - ipython_pygments_lexers=1.1.1\n",
    "  - isoduration=20.11.0\n",
    "  - jedi=0.19.2\n",
    "  - jinja2=3.1.6\n",
    "  - json5=0.12.1\n",
    "  - jsonpointer=3.0.0\n",
    "  - jsonschema=4.25.1\n",
    "  - jsonschema-specifications=2025.9.1\n",
    "  - jsonschema-with-format-nongpl=4.25.1\n",
    "  - jupyter-lsp=2.3.0\n",
    "  - jupyter_client=8.6.3\n",
    "  - jupyter_core=5.9.1\n",
    "  - jupyter_events=0.12.0\n",
    "  - jupyter_server=2.17.0\n",
    "  - jupyter_server_terminals=0.5.3\n",
    "  - jupyterlab=4.4.9\n",
    "  - jupyterlab_pygments=0.3.0\n",
    "  - jupyterlab_server=2.27.3\n",
    "  - keyutils=1.6.3\n",
    "  - krb5=1.21.3\n",
    "  - lark=1.3.0\n",
    "  - ld_impl_linux-64=2.44\n",
    "  - libedit=3.1.20250104\n",
    "  - libexpat=2.7.1\n",
    "  - libffi=3.4.6\n",
    "  - libgcc=15.2.0\n",
    "  - libgcc-ng=15.2.0\n",
    "  - libgomp=15.2.0\n",
    "  - liblzma=5.8.1\n",
    "  - libmpdec=4.0.0\n",
    "  - libsodium=1.0.20\n",
    "  - libsqlite=3.50.4\n",
    "  - libstdcxx=15.2.0\n",
    "  - libstdcxx-ng=15.2.0\n",
    "  - libuuid=2.41.2\n",
    "  - libzlib=1.3.1\n",
    "  - markupsafe=3.0.3\n",
    "  - matplotlib-inline=0.1.7\n",
    "  - mistune=3.1.4\n",
    "  - nbclient=0.10.2\n",
    "  - nbconvert-core=7.16.6\n",
    "  - nbformat=5.10.4\n",
    "  - ncurses=6.5\n",
    "  - nest-asyncio=1.6.0\n",
    "  - notebook-shim=0.2.4\n",
    "  - openssl=3.5.4\n",
    "  - overrides=7.7.0\n",
    "  - packaging=25.0\n",
    "  - pandocfilters=1.5.0\n",
    "  - parso=0.8.5\n",
    "  - pexpect=4.9.0\n",
    "  - pickleshare=0.7.5\n",
    "  - pip=25.2\n",
    "  - platformdirs=4.5.0\n",
    "  - prometheus_client=0.23.1\n",
    "  - prompt-toolkit=3.0.52\n",
    "  - psutil=7.1.0\n",
    "  - ptyprocess=0.7.0\n",
    "  - pure_eval=0.2.3\n",
    "  - pycparser=2.22\n",
    "  - pygments=2.19.2\n",
    "  - pysocks=1.7.1\n",
    "  - python=3.14.0\n",
    "  - python-dateutil=2.9.0.post0\n",
    "  - python-fastjsonschema=2.21.2\n",
    "  - python-gil=3.14.0\n",
    "  - python-json-logger=2.0.7\n",
    "  - python-tzdata=2025.2\n",
    "  - python_abi=3.14\n",
    "  - pytz=2025.2\n",
    "  - pyyaml=6.0.3\n",
    "  - pyzmq=27.1.0\n",
    "  - readline=8.2\n",
    "  - referencing=0.37.0\n",
    "  - requests=2.32.5\n",
    "  - rfc3339-validator=0.1.4\n",
    "  - rfc3986-validator=0.1.1\n",
    "  - rfc3987-syntax=1.1.0\n",
    "  - rpds-py=0.27.1\n",
    "  - send2trash=1.8.3\n",
    "  - setuptools=80.9.0\n",
    "  - six=1.17.0\n",
    "  - sniffio=1.3.1\n",
    "  - soupsieve=2.8\n",
    "  - stack_data=0.6.3\n",
    "  - terminado=0.18.1\n",
    "  - tinycss2=1.4.0\n",
    "  - tk=8.6.13\n",
    "  - tomli=2.3.0\n",
    "  - tornado=6.5.2\n",
    "  - traitlets=5.14.3\n",
    "  - typing-extensions=4.15.0\n",
    "  - typing_extensions=4.15.0\n",
    "  - typing_utils=0.1.0\n",
    "  - tzdata=2025b\n",
    "  - uri-template=1.3.0\n",
    "  - urllib3=2.5.0\n",
    "  - wcwidth=0.2.14\n",
    "  - webcolors=24.11.1\n",
    "  - webencodings=0.5.1\n",
    "  - websocket-client=1.9.0\n",
    "  - yaml=0.2.5\n",
    "  - zeromq=4.3.5\n",
    "  - zipp=3.23.0\n",
    "  - zstandard=0.25.0\n",
    "  - zstd=1.5.7\n",
    "  - pip:\n",
    "      - annotated-types==0.7.0\n",
    "      - ollama==0.6.0\n",
    "      - pydantic==2.12.3\n",
    "      - pydantic-core==2.41.4\n",
    "      - typing-inspection==0.4.2\n",
    "```\n",
    "\n",
    "Run the following command to build the conda environment:\n",
    "```bash\n",
    "conda env create -f ollama_env.yaml\n",
    "```\n",
    "\n",
    "## Starting JupyterLab\n",
    "Follow [`Guide: Using Jupyter on Ibex`](https://docs.hpc.kaust.edu.sa/soft_env/job_schd/slurm/interactive_jobs/jupyter.html#job-on-ibex) to start JupyterLab on a an Ibex GPU node using your conda environment instead of *'machine_learning'* module.\n",
    "\n",
    "By making the following changes to the Jupyter launch script:\n",
    "```bash\n",
    "#module load machine_learning/2024.01\n",
    "conda activate ollama_env\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4e0488-9b7e-4b87-8fcb-bf179f2e0530",
   "metadata": {},
   "source": [
    "## Starting the Ollama Server\n",
    "Start the OLLAMA REST API server using the following bash script in a terminal:\n",
    "\n",
    "The script has the following:\n",
    "- A user editable section, where the user defines Ollama models scratch directory.\n",
    "- The allocated port is saved in a temporary ollama_port.txt file, in order to be used in the Python notebook to read the assigned port to Ollama server.\n",
    "- Cleanup section in order to stop the singularity instance when the script is terminated.\n",
    "\n",
    "### User Modification Section\n",
    "- This section of the script is reserved for user-specific setup to set the directory where the Ollama models are pulled locally.\n",
    "- In the script, you will find a clearly marked block:\n",
    "    ```bash\n",
    "    # ------------------------------------\n",
    "    # START OF USER MODIFICATION SECTION\n",
    "    # ------------------------------------\n",
    "    ```\n",
    "    \n",
    "> Note: Do not modify other parts of the script unless you are sure, as they are required for correct execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb12f5c7-95d3-4195-8409-d09ab0e91cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running pre-start cleanup...\n",
      "Cleanup complete â€” ready to start new instance.\n",
      "Loading module for Singularity\n",
      "Singularity 3.9.7 modules now loaded\n",
      "OLLAMA PORT: 34321  -- Stored in ollama_port.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ollama-server-start.sh: line 9: singularity: command not found\n",
      "FATAL:   Image file already exists: \"ollama.sif\" - will not overwrite\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama server started. Logs at: /ibex/user/solimaay/scripts/jupyter/631115-ollama-sif/ibex-nb/ollama_server.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:    instance started successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['bash', 'ollama-server-start.sh'], returncode=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, subprocess\n",
    "\n",
    "script_content = \"\"\"\n",
    "#!/bin/bash\n",
    "\n",
    "# Pre-start cleanup: ensure no stale instances or files\n",
    "pre_cleanup() {\n",
    "    echo \"Running pre-start cleanup...\"\n",
    "\n",
    "    # 1. Stop any running Singularity instance with the same name\n",
    "    if singularity instance list | grep -q \"$SINGULARITY_INSTANCE_NAME\"; then\n",
    "        echo \"Stopping existing Singularity instance: $SINGULARITY_INSTANCE_NAME\"\n",
    "        singularity instance stop \"$SINGULARITY_INSTANCE_NAME\"\n",
    "    fi\n",
    "\n",
    "    # 2. Remove old temporary or state files\n",
    "    if [ -n \"$OLLAMA_PORT_TXT_FILE\" ] && [ -f \"$OLLAMA_PORT_TXT_FILE\" ]; then\n",
    "        echo \"Removing old port file: $OLLAMA_PORT_TXT_FILE\"\n",
    "        rm -f \"$OLLAMA_PORT_TXT_FILE\"\n",
    "    fi\n",
    "\n",
    "    if [ -n \"$OLLAMA_LOG_FILE\" ] && [ -f \"$OLLAMA_LOG_FILE\" ]; then\n",
    "        echo \"Removing old log file: $OLLAMA_LOG_FILE\"\n",
    "        rm -f \"$OLLAMA_LOG_FILE\"\n",
    "    fi\n",
    "\n",
    "    echo \"Cleanup complete â€” ready to start new instance.\"\n",
    "}\n",
    "\n",
    "# Cleanup process while exiting the server\n",
    "cleanup() {\n",
    "    echo \"ðŸ§¹   Cleaning up before exit...\"\n",
    "    # Put your exit commands here, e.g.:\n",
    "    rm -f $OLLAMA_PORT_TXT_FILE\n",
    "    # Remove the Singularity instance\n",
    "    singularity instance stop $SINGULARITY_INSTANCE_NAME\n",
    "}\n",
    "trap cleanup SIGINT  # Catch Ctrl+C (SIGINT) and run cleanup\n",
    "pre_cleanup\n",
    "\n",
    "# --------------------------------\n",
    "# START OF USER MODIFICATION SECTION\n",
    "# --------------------------------\n",
    "# Make target directory on /ibex/user/$USER/ollama_models_scratch to store your Ollama models\n",
    "export OLLAMA_MODELS_SCRATCH=/ibex/user/$USER/ollama_models_scratch\n",
    "# --------------------------------\n",
    "# END OF USER Editable Section\n",
    "# --------------------------------\n",
    "\n",
    "mkdir -p $OLLAMA_MODELS_SCRATCH\n",
    "\n",
    "SINGULARITY_INSTANCE_NAME='ollama'\n",
    "SINGULARITY_SIF_FILE=\"${SINGULARITY_INSTANCE_NAME}.sif\"\n",
    "OLLAMA_PORT_TXT_FILE='ollama_port.txt'\n",
    "LOG_FILE=$PWD/ollama_server.log\n",
    "\n",
    "# 2. Load Singularity module\n",
    "module load singularity\n",
    "\n",
    "# 3. Pull OLLAMA docker image\n",
    "singularity pull --name $SINGULARITY_SIF_FILE docker://ollama/ollama\n",
    "\n",
    "# 4. Change the default port for OLLAMA_HOST: (default 127.0.0.1:11434)\n",
    "export PORT=$(python -c 'import socket; s=socket.socket(); s.bind((\"\", 0)); print(s.getsockname()[1]); s.close()')\n",
    "\n",
    "# 5. Copy the assigned port, it will be required in the second part during working on the notebook.\n",
    "echo \"$PORT\" > $OLLAMA_PORT_TXT_FILE\n",
    "\n",
    "echo \"OLLAMA PORT: $PORT  -- Stored in $OLLAMA_PORT_TXT_FILE\"\n",
    "\n",
    "# 6. Define the OLLAMA Host\n",
    "export SINGULARITYENV_OLLAMA_HOST=127.0.0.1:$PORT\n",
    "\n",
    "# 7. Change the default model directory stored: \n",
    "export SINGULARITYENV_OLLAMA_MODELS=$OLLAMA_MODELS_SCRATCH\n",
    "\n",
    "# 8. Create an Instance:\n",
    "singularity instance start --nv -B \"/ibex/user:/ibex/user\" $SINGULARITY_SIF_FILE $SINGULARITY_INSTANCE_NAME\n",
    "\n",
    "# 7. Run the OLLAMA REST API server on the background\n",
    "nohup singularity exec instance://$SINGULARITY_INSTANCE_NAME bash -c \"ollama serve\" > $LOG_FILE 2>&1 &\n",
    "echo \"Ollama server started. Logs at: $LOG_FILE\"\n",
    "\"\"\"\n",
    "\n",
    "# Write script file\n",
    "script_path = \"ollama-server-start.sh\"\n",
    "with open(script_path, \"w\") as f:\n",
    "    f.write(script_content)\n",
    "os.chmod(script_path, 0o755)\n",
    "\n",
    "# Run script\n",
    "subprocess.run([\"bash\", script_path])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e87946-05e4-4f00-b71a-a00cd89f31c8",
   "metadata": {},
   "source": [
    "## Using Ollama Packages Requests\n",
    "Follow the following Python notebook below, it contains the codes for:\n",
    "- Initialization Setup.\n",
    "- List local models.\n",
    "- Pull models.\n",
    "- Testing connection to the Ollama server.\n",
    "- Chat with the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9f194e-c96a-435f-8008-99b63126c62f",
   "metadata": {},
   "source": [
    "### 1. Initialization\n",
    "1. Define the base URL for the remote Ollama Server.\n",
    "2. Testing the Ollama server connectivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "965e31e2-2c23-48ce-bad0-d73d19e0383f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from ollama import AsyncClient\n",
    "from typing import List, Dict\n",
    "\n",
    "MAX_CONCURRENT = 2  # limit to avoid GPU overload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f97d9b1-88ba-4e32-8613-127e9d292634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://127.0.0.1:34321\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "with open(\"ollama_port.txt\") as f :\n",
    "    PORT = f.read().strip()\n",
    "    \n",
    "BASE_URL=f\"http://127.0.0.1:{PORT}\"\n",
    "print(BASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb259eaa-388a-4157-8f70-69ffdd80a931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama is running! 200\n"
     ]
    }
   ],
   "source": [
    "# Testing the server connectivity\n",
    "import requests\n",
    "\n",
    "try:\n",
    "    r = requests.get(BASE_URL)\n",
    "    print(\"Ollama is running!\", r.status_code)\n",
    "except requests.ConnectionError as e:\n",
    "    print(\"Ollama is NOT reachable:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d42bfc3-bd60-4c96-984d-54bf4112e56c",
   "metadata": {},
   "source": [
    "### 2. Get a List of Local Models\n",
    "- Get a list of locally available Ollama models.\n",
    "- Locally available models are located under path: */ibex/user/$USER/ollama_models_scratch*\n",
    "- To change the location for pulled models, modify the variable *OLLAMA_MODELS_SCRATCH* in the script*start_ollama_server.sh*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdbdc6e8-3d93-4063-b186-54a7afba8f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Client\n",
    "client = Client(\n",
    "  host=BASE_URL,\n",
    ")\n",
    "\n",
    "def get_local_models():\n",
    "    \"\"\"\n",
    "    Returns a list of locally available Ollama Models.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of model names as strings\n",
    "    \"\"\"\n",
    "    models = [model['model'] for model in client.list()['models']]\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a877e166-05a6-43fb-aed5-f817c1cd7c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['phi3:3.8b', 'qwen3:0.6b', 'gemma3:270m']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Usage\n",
    "get_local_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24aea834-7c62-47c3-9381-b31d1e281367",
   "metadata": {},
   "source": [
    "### 3. Pull The Model\n",
    "- To pull a specific model, use *pull* method.\n",
    "- Please refer to [Ollama Library](https://ollama.com/library) to check available models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "49d338e9-756a-4a79-b578-12201bc3c397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProgressResponse(status='success', completed=None, total=None, digest=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pull the required models\n",
    "client.pull(\"qwen3:0.6b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5def6220-da2c-4ae7-95fb-ecc8bcd71878",
   "metadata": {},
   "source": [
    "### 4. Running Batch Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c568725e-4b07-42cc-b625-9a63e9a95cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def ensure_model_exists(client: AsyncClient, model: str):\n",
    "    \"\"\"\n",
    "    Ensure that a specified Ollama model is available locally.  \n",
    "    If the model is not installed, it will be pulled from the server.\n",
    "\n",
    "    Args:\n",
    "        client (AsyncClient): An instance of the AsyncClient connected to the Ollama server.\n",
    "        model (str): Name of the model to check and pull if necessary.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If pulling or checking the model fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if the model is already available\n",
    "        await client.show(model)\n",
    "        print(f\"Model {model} already available locally.\")\n",
    "    except Exception:\n",
    "        # Pull the model if it does not exist\n",
    "        print(f\"Pulling model {model}...\")\n",
    "        async for progress in await client.pull(model, stream=True):\n",
    "            status = progress.get(\"status\", \"\")\n",
    "            if \"completed\" in status.lower():\n",
    "                print(f\"Pulled {model} successfully.\")\n",
    "        print(f\"Model {model} is now ready for use.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b004284-a295-4fb3-b8fa-4c90b2036ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def query_model_async(client: AsyncClient, model: str, prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Send a single prompt to a specified Ollama model asynchronously and return the full response.\n",
    "\n",
    "    Args:\n",
    "        client (AsyncClient): An instance of AsyncClient connected to the Ollama server.\n",
    "        model (str): Name of the Ollama model to query.\n",
    "        prompt (str): The user input to send to the model.\n",
    "\n",
    "    Returns:\n",
    "        str: The complete response text from the model.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If the chat request fails.\n",
    "    \"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = \"\"\n",
    "    \n",
    "    async for chunk in await client.chat(model=model, messages=messages, stream=True):\n",
    "        if chunk.get(\"message\") and \"content\" in chunk[\"message\"]:\n",
    "            response += chunk[\"message\"][\"content\"]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f0128cd-ab09-4a1c-9346-1590f0649863",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_batch(models: List[str], prompt: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Run multiple model inferences concurrently while limiting active requests.\n",
    "\n",
    "    This function uses asynchronous concurrency control to efficiently query \n",
    "    multiple models in parallel, ensuring that no more than `max_concurrent` \n",
    "    requests are active at a time. Each model is checked for availability before \n",
    "    being queried, and missing models are automatically pulled.\n",
    "\n",
    "    Args:\n",
    "        models (List[str]): A list of model names to query.\n",
    "        prompt (str): The user input or question to be sent to each model.\n",
    "        base_url (str, optional): The base URL of the Ollama API endpoint.\n",
    "        max_concurrent (int, optional): The maximum number of models to run concurrently.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, str]: A dictionary mapping model names to their response text.\n",
    "    \"\"\"\n",
    "    client = AsyncClient(host=BASE_URL)\n",
    "    semaphore = asyncio.Semaphore(MAX_CONCURRENT)\n",
    "\n",
    "    async def safe_query(model):\n",
    "        async with semaphore:\n",
    "            # Ensure model is available before querying\n",
    "            await ensure_model_exists(client, model)\n",
    "            print(f\"Running {model}...\")\n",
    "            result = await query_model_async(client, model, prompt)\n",
    "            print(f\"Done: {model}\")\n",
    "            return model, result\n",
    "\n",
    "    results = await asyncio.gather(*(safe_query(m) for m in models))\n",
    "    return dict(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0eaa0a66-0dc1-4080-93d7-67e97678b2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def judge_model_responses(\n",
    "    client: AsyncClient,\n",
    "    judge_model: str,\n",
    "    responses: Dict[str, str],\n",
    "    criteria: str\n",
    ") -> Dict[str, Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Use an LLM to evaluate the outputs of other models according to a given criteria.\n",
    "\n",
    "    Args:\n",
    "        client (AsyncClient): An instance of AsyncClient connected to the Ollama server.\n",
    "        responses (Dict[str, str]): A dictionary mapping model names to their outputs.\n",
    "        criteria (str): Evaluation criteria to guide the judging process.\n",
    "        judge_model (str, optional): The model used as the judge. Defaults to \"llama3\".\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Dict[str, str]]: A dictionary mapping each evaluated model to its judgment,\n",
    "        containing keys like \"evaluation\" with the judge's reasoning and score.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If model evaluation fails or judge model cannot be ensured.\n",
    "    \"\"\"\n",
    "    judged = {}\n",
    "    \n",
    "    # Ensure the judge model exists locally\n",
    "    await ensure_model_exists(client, judge_model)\n",
    "\n",
    "    for model, answer in responses.items():\n",
    "        judge_prompt = f\"\"\"\n",
    "You are an impartial judge. Evaluate the following answer according to {criteria}.\n",
    "\n",
    "Answer:\n",
    "{answer}\n",
    "\n",
    "Give:\n",
    "- Reasoning\n",
    "- Score from 1 to 10\n",
    "\"\"\"\n",
    "        eval_resp = await query_model_async(client, judge_model, judge_prompt)\n",
    "        judged[model] = {\"evaluation\": eval_resp}\n",
    "        \n",
    "    return judged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "892ba43c-7632-4847-862a-32e687e39e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    # Define List of Models [model_1, model_2, ...]\n",
    "    models = ['qwen3:0.6b', 'gemma3:270m']\n",
    "    # Define the Judge LLM model\n",
    "    judge_model = \"phi3:3.8b\"\n",
    "    # Define the prompt\n",
    "    prompt = \"What is 1+1 equal?\"\n",
    "    client = AsyncClient(host=BASE_URL)\n",
    "\n",
    "    # 1. Run models (with auto-pull if missing)\n",
    "    responses = await run_batch(models, prompt)\n",
    "\n",
    "    # 2. Judge responses\n",
    "    evaluations = await judge_model_responses(client, judge_model, responses, \"clarity, correctness, and conciseness\")\n",
    "\n",
    "    # 3. Display\n",
    "    for m in models:\n",
    "        print(f\"\\n--- {m} ---\")\n",
    "        print(\"Response:\\n\", responses[m])\n",
    "        print(\"Evaluation:\\n\", evaluations[m][\"evaluation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "689e1ab9-712e-4dc4-9850-94e089c61675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model qwen3:0.6b already available locally.\n",
      "Running qwen3:0.6b...\n",
      "Model gemma3:270m already available locally.\n",
      "Running gemma3:270m...\n",
      "Done: gemma3:270m\n",
      "Done: qwen3:0.6b\n",
      "Model phi3:3.8b already available locally.\n",
      "\n",
      "--- qwen3:0.6b ---\n",
      "Response:\n",
      " 1 + 1 equals 2.\n",
      "Evaluation:\n",
      " Reasoning: The provided response is succinct, accurate in terms of basic arithmetic principles (one plus one indeed yields two), and unambiguous due to the straightforward language used. Therefore, it effectively conveys a simple mathematical truth with minimal words required.\n",
      "\n",
      "Score: I would give this answer an 8 out of 10 for clarity, correctness, and conciseness. The response could potentially be enhanced by providing context or stating that \"one plus one equals two\" explicitly defines the operation performed (addition), yet it remains unambiguous without such additions. However, as a standalone statement about an arithmetic fact, its simplicity is commendable, warranting this high score with just slight room for improvement.\n",
      "\n",
      "--- gemma3:270m ---\n",
      "Response:\n",
      " 1 + 1 = 2\n",
      "\n",
      "Evaluation:\n",
      " \n",
      "Reasoning: The provided statement presents a basic arithmetic operation which is the addition of two integers, both equal to one. This expression directly follows mathematical conventions for representing simple summation and provides an immediate answer without any ambiguity or extraneous information. As such, it adheres well to principles requiring clarity, correctness, and conciseness in presenting basic calculations within a mathematics context.\n",
      "\n",
      "Score: 10/10 - The statement is clear, completely accurate for the operation performed (simple addition), and succinctly conveys the necessary information without superfluous details or unnecessary complexity that would detract from its quality as an answer to this mathematical expression problem.\n"
     ]
    }
   ],
   "source": [
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4553778-4c3d-4777-8a83-a52b49c15a3c",
   "metadata": {},
   "source": [
    "## Stop the Ollama Server\n",
    "Make sure to stop the Ollama server by terminating the Singularity container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cab1430-603e-4d4f-9fa3-f271fc1be9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "def stop_singularity_instance(instance_name=\"ollama\", log_file=None, port_file=None):\n",
    "    \"\"\"\n",
    "    Gracefully stop a running Singularity instance by name, \n",
    "    and optionally remove associated log or port files.\n",
    "    \"\"\"\n",
    "    print(f\"Checking for Singularity instance: {instance_name}\")\n",
    "\n",
    "    # 1. Check if instance is running\n",
    "    try:\n",
    "\n",
    "        result = subprocess.run(\n",
    "            'bash -lc \"module load singularity 2>/dev/null || true; singularity instance list\"',\n",
    "            shell=True,\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "\n",
    "        if instance_name not in result.stdout:\n",
    "            print(f\"No running instance named '{instance_name}' found.\")\n",
    "        else:\n",
    "            print(f\"Instance '{instance_name}' is running. Attempting to stop it...\")\n",
    "            stop_result = subprocess.run(\n",
    "                f'bash -lc \"module load singularity 2>/dev/null || true; singularity instance stop {instance_name}\"',\n",
    "                shell=True,\n",
    "                capture_output=True,\n",
    "                text=True\n",
    "            )\n",
    "            if stop_result.returncode == 0:\n",
    "                print(f\"Singularity instance '{instance_name}' stopped successfully.\")\n",
    "            else:\n",
    "                print(f\"Warning: Failed to stop instance '{instance_name}'.\")\n",
    "                print(stop_result.stderr)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"Singularity command not found. Ensure it's installed and in PATH.\")\n",
    "        return\n",
    "\n",
    "    # 2. Optional cleanup for files\n",
    "    if port_file and os.path.exists(port_file):\n",
    "        os.remove(port_file)\n",
    "        print(f\"Removed port file: {port_file}\")\n",
    "\n",
    "    if log_file and os.path.exists(log_file):\n",
    "        os.remove(log_file)\n",
    "        print(f\"Removed log file: {log_file}\")\n",
    "\n",
    "    print(\"Cleanup complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "379b154e-86ce-44dc-b0a8-45adb4b117cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for Singularity instance: ollama\n",
      "Instance 'ollama' is running. Attempting to stop it...\n",
      "Singularity instance 'ollama' stopped successfully.\n",
      "Cleanup complete.\n"
     ]
    }
   ],
   "source": [
    "stop_singularity_instance(\n",
    "    instance_name=\"ollama\",\n",
    "    log_file=os.path.expandvars(\"$PWD/ollama_server.log\"),\n",
    "    port_file=os.path.expandvars(\"$PWD/ollama_port.txt\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9158bab4-877c-482d-862c-4edcbe7d746b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
