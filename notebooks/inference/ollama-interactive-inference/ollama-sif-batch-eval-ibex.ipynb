{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0ea5656",
   "metadata": {},
   "source": [
    "# Ollama Batch Evaluation Guide (LLM-as-a-Judge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9822d045",
   "metadata": {},
   "source": [
    "> This page was generated from [ollama-interactive-inference/ollama-sif-batch-eval-ibex.ipynb](https://github.com/kaust-rccl/Data-science-onboarding/tree/main/notebooks/inference/ollama-interactive-inference/ollama-sif-batch-eval-ibex.ipynb). You can [view or download notebook](https://github.com/kaust-rccl/Data-science-onboarding/tree/main/notebooks/inference/ollama-interactive-inference/ollama-sif-batch-eval-ibex.ipynb). Or [view it on nbviewer](https://nbviewer.org/github/kaust-rccl/Data-science-onboarding/tree/main/notebooks/inference/ollama-interactive-inference/ollama-sif-batch-eval-ibex.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db18baaa",
   "metadata": {},
   "source": [
    "## Objective\n",
    "This guide helps you evaluate multiple model responses automatically using Ollamaâ€™s batch evaluation feature. Instead of manually scoring outputs, an LLM acts as a judge, comparing predictions against reference answers or quality criteria you define.\n",
    "\n",
    "## Initial Setup\n",
    "If you haven't installed conda yet, please follow :ref:`conda_ibex_` to get started.\n",
    "\n",
    "After conda has been installed, save the following environment yaml file on Ibex under the name ``ollama_env.yaml``\n",
    "\n",
    "```yml\n",
    "name: ollama_env\n",
    "channels:\n",
    "- conda-forge\n",
    "dependencies:\n",
    "- _libgcc_mutex=0.1\n",
    "- _openmp_mutex=4.5\n",
    "- bzip2=1.0.8\n",
    "- ca-certificates=2025.7.14\n",
    "- icu=75.1\n",
    "- ld_impl_linux-64=2.44\n",
    "- libexpat=2.7.1\n",
    "- libffi=3.4.6\n",
    "- libgcc=15.1.0\n",
    "- libgcc-ng=15.1.0\n",
    "- libgomp=15.1.0\n",
    "- liblzma=5.8.1\n",
    "- libmpdec=4.0.0\n",
    "- libsqlite=3.50.3\n",
    "- libstdcxx=15.1.0\n",
    "- libstdcxx-ng=15.1.0\n",
    "- libuuid=2.38.1\n",
    "- libzlib=1.3.1\n",
    "- ncurses=6.5\n",
    "- openssl=3.5.1\n",
    "- pip=25.1.1\n",
    "- python=3.13.5\n",
    "- python_abi=3.13\n",
    "- readline=8.2\n",
    "- tk=8.6.13\n",
    "- tzdata=2025b\n",
    "- pip:\n",
    "    - annotated-types==0.7.0\n",
    "    - anyio==4.9.0\n",
    "    - argon2-cffi==25.1.0\n",
    "    - argon2-cffi-bindings==21.2.0\n",
    "    - arrow==1.3.0\n",
    "    - asttokens==3.0.0\n",
    "    - async-lru==2.0.5\n",
    "    - attrs==25.3.0\n",
    "    - babel==2.17.0\n",
    "    - beautifulsoup4==4.13.4\n",
    "    - bleach==6.2.0\n",
    "    - certifi==2025.7.14\n",
    "    - cffi==1.17.1\n",
    "    - charset-normalizer==3.4.2\n",
    "    - comm==0.2.2\n",
    "    - debugpy==1.8.15\n",
    "    - decorator==5.2.1\n",
    "    - defusedxml==0.7.1\n",
    "    - executing==2.2.0\n",
    "    - fastjsonschema==2.21.1\n",
    "    - fqdn==1.5.1\n",
    "    - h11==0.16.0\n",
    "    - httpcore==1.0.9\n",
    "    - httpx==0.28.1\n",
    "    - idna==3.10\n",
    "    - ipykernel==6.30.0\n",
    "    - ipython==9.4.0\n",
    "    - ipython-pygments-lexers==1.1.1\n",
    "    - ipywidgets==8.1.7\n",
    "    - isoduration==20.11.0\n",
    "    - jedi==0.19.2\n",
    "    - jinja2==3.1.6\n",
    "    - json5==0.12.0\n",
    "    - jsonpointer==3.0.0\n",
    "    - jsonschema==4.25.0\n",
    "    - jsonschema-specifications==2025.4.1\n",
    "    - jupyter==1.1.1\n",
    "    - jupyter-client==8.6.3\n",
    "    - jupyter-console==6.6.3\n",
    "    - jupyter-core==5.8.1\n",
    "    - jupyter-events==0.12.0\n",
    "    - jupyter-lsp==2.2.6\n",
    "    - jupyter-server==2.16.0\n",
    "    - jupyter-server-terminals==0.5.3\n",
    "    - jupyterlab==4.4.5\n",
    "    - jupyterlab-pygments==0.3.0\n",
    "    - jupyterlab-server==2.27.3\n",
    "    - jupyterlab-widgets==3.0.15\n",
    "    - lark==1.2.2\n",
    "    - markupsafe==3.0.2\n",
    "    - matplotlib-inline==0.1.7\n",
    "    - mistune==3.1.3\n",
    "    - nbclient==0.10.2\n",
    "    - nbconvert==7.16.6\n",
    "    - nbformat==5.10.4\n",
    "    - nest-asyncio==1.6.0\n",
    "    - notebook==7.4.4\n",
    "    - notebook-shim==0.2.4\n",
    "    - ollama==0.5.1\n",
    "    - overrides==7.7.0\n",
    "    - packaging==25.0\n",
    "    - pandocfilters==1.5.1\n",
    "    - parso==0.8.4\n",
    "    - pexpect==4.9.0\n",
    "    - platformdirs==4.3.8\n",
    "    - prometheus-client==0.22.1\n",
    "    - prompt-toolkit==3.0.51\n",
    "    - psutil==7.0.0\n",
    "    - ptyprocess==0.7.0\n",
    "    - pure-eval==0.2.3\n",
    "    - pycparser==2.22\n",
    "    - pydantic==2.11.7\n",
    "    - pydantic-core==2.33.2\n",
    "    - pygments==2.19.2\n",
    "    - python-dateutil==2.9.0.post0\n",
    "    - python-json-logger==3.3.0\n",
    "    - pyyaml==6.0.2\n",
    "    - pyzmq==27.0.0\n",
    "    - referencing==0.36.2\n",
    "    - requests==2.32.4\n",
    "    - rfc3339-validator==0.1.4\n",
    "    - rfc3986-validator==0.1.1\n",
    "    - rfc3987-syntax==1.1.0\n",
    "    - rpds-py==0.26.0\n",
    "    - send2trash==1.8.3\n",
    "    - setuptools==80.9.0\n",
    "    - six==1.17.0\n",
    "    - sniffio==1.3.1\n",
    "    - soupsieve==2.7\n",
    "    - stack-data==0.6.3\n",
    "    - terminado==0.18.1\n",
    "    - tinycss2==1.4.0\n",
    "    - tornado==6.5.1\n",
    "    - traitlets==5.14.3\n",
    "    - types-python-dateutil==2.9.0.20250708\n",
    "    - typing-extensions==4.14.1\n",
    "    - typing-inspection==0.4.1\n",
    "    - uri-template==1.3.0\n",
    "    - urllib3==2.5.0\n",
    "    - wcwidth==0.2.13\n",
    "    - webcolors==24.11.1\n",
    "    - webencodings==0.5.1\n",
    "    - websocket-client==1.8.0\n",
    "    - widgetsnbextension==4.0.14\n",
    "```\n",
    "\n",
    "Run the following command to build the conda environment:\n",
    "```bash\n",
    "conda env create -f ollama_env.yaml\n",
    "```\n",
    "\n",
    "## Starting JupyterLab\n",
    "Follow [`using_jupyter`](../../jupyter) to start JupyterLab on a an Ibex GPU node Using your conda environment instead of ``machine_learning`` module.\n",
    "By making the following changes to the Jupyter launch script.\n",
    "```bash\n",
    "#module load machine_learning/2024.01\n",
    "conda activate ollama_en\n",
    "```\n",
    "\n",
    "## Starting The Ollama Server\n",
    "Start the OLLAMA REST API server using the following bash script in a terminal:\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "# Cleanup process while exiting the server\n",
    "cleanup() {\n",
    "    echo \"ðŸ§¹   Cleaning up before exit...\"\n",
    "    # Put your exit commands here, e.g.:\n",
    "    rm -f $OLLAMA_PORT_TXT_FILE\n",
    "    # Remove the Singularity instance\n",
    "    singularity instance stop $SINGULARITY_INSTANCE_NAME\n",
    "}\n",
    "trap cleanup SIGINT  # Catch Ctrl+C (SIGINT) and run cleanup\n",
    "#trap cleanup EXIT    # Also run on any script exit\n",
    "\n",
    "# User Editable Section\n",
    "# 1. Make target directory on /ibex/user/$USER/ollama_models_scratch to store your Ollama models\n",
    "export OLLAMA_MODELS_SCRATCH=/ibex/user/$USER/ollama_models_scratch\n",
    "mkdir -p $OLLAMA_MODELS_SCRATCH\n",
    "# End of User Editable Section\n",
    "\n",
    "SINGULARITY_INSTANCE_NAME=\"ollama\"\n",
    "OLLAMA_PORT_TXT_FILE='ollama_port.txt'\n",
    "\n",
    "# 2. Load Singularity module\n",
    "module load singularity\n",
    "\n",
    "# 3. Pull OLLAMA docker image\n",
    "singularity pull docker://ollama/ollama\n",
    "\n",
    "# 4. Change the default port for OLLAMA_HOST: (default 127.0.0.1:11434)\n",
    "export PORT=$(python -c 'import socket; s=socket.socket(); s.bind((\"\", 0)); print(s.getsockname()[1]); s.close()')\n",
    "\n",
    "# 5. Copy the assigned port, it will be required in the second part during working on the notebook.\n",
    "echo \"$PORT\" > $OLLAMA_PORT_TXT_FILE\n",
    "\n",
    "echo \"OLLAMA PORT: $PORT  -- Stored in $OLLAMA_PORT_TXT_FILE\"\n",
    "\n",
    "# 6. Define the OLLAMA Host\n",
    "export SINGULARITYENV_OLLAMA_HOST=127.0.0.1:$PORT\n",
    "\n",
    "# 7. Change the default model directory stored: (default ~/.ollama/models/manifests/registry.ollama.ai/library)\n",
    "export SINGULARITYENV_OLLAMA_MODELS=$OLLAMA_MODELS_SCRATCH\n",
    "\n",
    "# 8. Create an Instance:\n",
    "singularity instance start --nv -B \"/ibex/user:/ibex/user\" ollama_latest.sif $SINGULARITY_INSTANCE_NAME\n",
    "\n",
    "# 7. Run the OLLAMA REST API server on the background\n",
    "singularity exec instance://$SINGULARITY_INSTANCE_NAME bash -c \"ollama serve\"\n",
    "```\n",
    "\n",
    "> Note: Save the above script in a file called start_ollama_server.sh\n",
    "\n",
    "```bash\n",
    "# Run the script to start the Ollama server.\n",
    "bash start_ollama_server.sh\n",
    "```\n",
    "\n",
    "The script has the following:\n",
    "- A user editable section, where the user defines [Ollama models scratch directory].\n",
    "- The allocated port is saved in a temporary ollama_port.txt file, in order to be used in the Python notebook to read the assigned port to Ollama server.\n",
    "- Cleanup section in order to stop the singularity instance when the script is terminated with CTRL+C.\n",
    "\n",
    "## Using REST API Requests\n",
    "Follow the following Python notebook below, it contains the codes for [Testing connection to the Ollama server, List local models, Pull models, Chat with the models]:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6fca72",
   "metadata": {},
   "source": [
    "This guide helps you evaluate multiple model responses automatically using Ollamaâ€™s batch evaluation feature. Instead of manually scoring outputs, an LLM acts as a judge, comparing predictions against reference answers or quality criteria you define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "965e31e2-2c23-48ce-bad0-d73d19e0383f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from ollama import AsyncClient\n",
    "from typing import List, Dict\n",
    "\n",
    "MAX_CONCURRENT = 2  # limit to avoid GPU overload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f97d9b1-88ba-4e32-8613-127e9d292634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://127.0.0.1:40743\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "with open(\"ollama_port.txt\") as f :\n",
    "    PORT = f.read().strip()\n",
    "    \n",
    "BASE_URL=f\"http://127.0.0.1:{PORT}\"\n",
    "print(BASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb259eaa-388a-4157-8f70-69ffdd80a931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama is running! 200\n"
     ]
    }
   ],
   "source": [
    "# Testing the server connectivity\n",
    "import requests\n",
    "\n",
    "try:\n",
    "    r = requests.get(BASE_URL)\n",
    "    print(\"Ollama is running!\", r.status_code)\n",
    "except requests.ConnectionError as e:\n",
    "    print(\"Ollama is NOT reachable:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdbdc6e8-3d93-4063-b186-54a7afba8f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemma3:latest\n",
      "qwen3:latest\n",
      "llama3:latest\n",
      "deepseek-r1:1.5b\n"
     ]
    }
   ],
   "source": [
    "from ollama import Client\n",
    "client = Client(\n",
    "  host=BASE_URL,\n",
    ")\n",
    "\n",
    "def get_local_models():\n",
    "    for model in client.list()['models']:\n",
    "        print(model['model'])\n",
    "\n",
    "get_local_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49d338e9-756a-4a79-b578-12201bc3c397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"status\":\"pulling manifest\"}\n",
      "{\"status\":\"pulling 6a0746a1ec1a\",\"digest\":\"sha256:6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa\",\"total\":4661211424,\"completed\":4661211424}\n",
      "{\"status\":\"pulling 4fa551d4f938\",\"digest\":\"sha256:4fa551d4f938f68b8c1e6afa9d28befb70e3f33f75d0753248d530364aeea40f\",\"total\":12403,\"completed\":12403}\n",
      "{\"status\":\"pulling 8ab4849b038c\",\"digest\":\"sha256:8ab4849b038cf0abc5b1c9b8ee1443dca6b93a045c2272180d985126eb40bf6f\",\"total\":254,\"completed\":254}\n",
      "{\"status\":\"pulling 577073ffcc6c\",\"digest\":\"sha256:577073ffcc6ce95b9981eacc77d1039568639e5638e83044994560d9ef82ce1b\",\"total\":110,\"completed\":110}\n",
      "{\"status\":\"pulling 3f8eb4da87fa\",\"digest\":\"sha256:3f8eb4da87fa7a3c9da615036b0dc418d31fef2a30b115ff33562588b32c691d\",\"total\":485,\"completed\":485}\n",
      "{\"status\":\"verifying sha256 digest\"}\n",
      "{\"status\":\"writing manifest\"}\n",
      "{\"status\":\"success\"}\n"
     ]
    }
   ],
   "source": [
    "# Pull the required model\n",
    "# You can check the available models in: https://ollama.com/library\n",
    "import requests\n",
    "\n",
    "def pull_model(model_name, base_url=BASE_URL):\n",
    "    url = f\"{base_url}/api/pull\"\n",
    "    response = requests.post(url, json={\"name\": model_name}, stream=True)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(\"âŒ Failed to pull model:\", response.text)\n",
    "        return\n",
    "\n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            decoded = line.decode(\"utf-8\")\n",
    "            print(decoded)\n",
    "\n",
    "# Usage\n",
    "pull_model(\"llama3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c568725e-4b07-42cc-b625-9a63e9a95cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def ensure_model(client: AsyncClient, model: str):\n",
    "    \"\"\"Check if model exists, if not, pull it.\"\"\"\n",
    "    try:\n",
    "        # Try showing model details (will fail if not installed)\n",
    "        await client.show(model)\n",
    "        print(f\"âœ… Model {model} already available.\")\n",
    "    except Exception:\n",
    "        print(f\"ðŸ“¥ Pulling model {model}...\")\n",
    "        async for progress in await client.pull(model, stream=True):\n",
    "            status = progress.get(\"status\")\n",
    "            if \"completed\" in status.lower():\n",
    "                print(f\"âœ… Pulled {model}\")\n",
    "        print(f\"âœ… Model {model} is now ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b004284-a295-4fb3-b8fa-4c90b2036ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def query_model(client: AsyncClient, model: str, prompt: str) -> str:\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = \"\"\n",
    "    async for chunk in await client.chat(model=model, messages=messages, stream=True):\n",
    "        if chunk.get(\"message\"):\n",
    "            response += chunk[\"message\"][\"content\"]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f0128cd-ab09-4a1c-9346-1590f0649863",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_batch(models: List[str], prompt: str) -> Dict[str, str]:\n",
    "    client = AsyncClient(host=BASE_URL)\n",
    "    semaphore = asyncio.Semaphore(MAX_CONCURRENT)\n",
    "\n",
    "    async def safe_query(model):\n",
    "        async with semaphore:\n",
    "            # Ensure model is available before querying\n",
    "            await ensure_model(client, model)\n",
    "            print(f\"ðŸ”„ Running {model}...\")\n",
    "            result = await query_model(client, model, prompt)\n",
    "            print(f\"âœ… Done: {model}\")\n",
    "            return model, result\n",
    "\n",
    "    results = await asyncio.gather(*(safe_query(m) for m in models))\n",
    "    return dict(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0eaa0a66-0dc1-4080-93d7-67e97678b2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def judge_responses(client: AsyncClient, responses: Dict[str, str], criteria: str) -> Dict[str, Dict]:\n",
    "    \"\"\"Use an LLM to evaluate other models' outputs.\"\"\"\n",
    "    judged = {}\n",
    "    # Ensure judge model exists\n",
    "    judge_model = \"llama3\"\n",
    "    await ensure_model(client, judge_model)\n",
    "\n",
    "    for model, answer in responses.items():\n",
    "        judge_prompt = f\"\"\"\n",
    "You are an impartial judge. Evaluate the following answer according to {criteria}.\n",
    "\n",
    "Answer:\n",
    "{answer}\n",
    "\n",
    "Give:\n",
    "- Reasoning\n",
    "- Score from 1 to 10\n",
    "\"\"\"\n",
    "        eval_resp = await query_model(client, judge_model, judge_prompt)\n",
    "        judged[model] = {\"evaluation\": eval_resp}\n",
    "    return judged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "892ba43c-7632-4847-862a-32e687e39e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    models = [\"qwen3\", \"llama3\", \"gemma3\"]\n",
    "    prompt = \"What is 1+1 equal?\"\n",
    "    client = AsyncClient(host=BASE_URL)\n",
    "\n",
    "    # 1. Run models (with auto-pull if missing)\n",
    "    responses = await run_batch(models, prompt)\n",
    "\n",
    "    # 2. Judge responses\n",
    "    evaluations = await judge_responses(client, responses, \"clarity, correctness, and conciseness\")\n",
    "\n",
    "    # 3. Display\n",
    "    for m in models:\n",
    "        print(f\"\\n--- {m} ---\")\n",
    "        print(\"Response:\\n\", responses[m])\n",
    "        print(\"Evaluation:\\n\", evaluations[m][\"evaluation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "689e1ab9-712e-4dc4-9850-94e089c61675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model qwen3 already available.\n",
      "ðŸ”„ Running qwen3...\n",
      "âœ… Model llama3 already available.\n",
      "ðŸ”„ Running llama3...\n",
      "âœ… Done: llama3\n",
      "âœ… Model gemma3 already available.\n",
      "ðŸ”„ Running gemma3...\n",
      "âœ… Done: gemma3\n",
      "âœ… Done: qwen3\n",
      "âœ… Model llama3 already available.\n",
      "\n",
      "--- qwen3 ---\n",
      "Response:\n",
      " <think>\n",
      "Okay, the user is asking \"What is 1+1 equal?\" Let me think. First, I need to confirm if this is a straightforward math question. The user might be testing basic arithmetic or maybe they have a trick question in mind. Let me start by recalling that in standard arithmetic, 1 plus 1 equals 2. That's the fundamental addition in base 10.\n",
      "\n",
      "Wait, could there be any alternative interpretations? For example, in some contexts, like binary numbers, 1 + 1 is 10, which is 2 in decimal. But the question doesn't specify a different numeral system, so I should assume base 10 unless told otherwise. Also, in programming or logic, sometimes operations can be different, but again, the question seems simple.\n",
      "\n",
      "Is there a possibility of a trick question? Maybe the user is trying to see if I know about different mathematical systems or if they're a student learning basic math. Alternatively, they might be testing my understanding of different contexts where 1+1 isn't 2. For example, in some abstract algebra structures, like modulo arithmetic, 1+1 could be 0 if we're working modulo 2. But again, without context, the default assumption is standard arithmetic.\n",
      "\n",
      "Another angle: maybe the user is using 1+1 in a different language or cultural context. For instance, in some languages, the word for \"one\" might be different, but the numerical value remains the same. However, the question is in English, so that's probably not relevant here.\n",
      "\n",
      "I should also consider if the user is a child just learning addition, in which case the answer is straightforward. Or if they're a programmer, maybe they're thinking about bitwise operations. Let me check: in binary, 1 + 1 is 10, which is 2 in decimal. So both interpretations are possible, but the question doesn't specify. However, the most common and basic answer is 2.\n",
      "\n",
      "Wait, could there be any other interpretations? Like in set theory, the union of two singleton sets would have two elements, so 1 + 1 = 2. In cardinal numbers, yes. So regardless of the context, unless specified otherwise, 1+1 equals 2. \n",
      "\n",
      "I should also make sure that there's no trick or pun involved. For example, \"1+1\" could be a play on words, but I don't see an immediate pun here. The user might be testing my knowledge of basic math, so the answer is 2. \n",
      "\n",
      "To sum up, the answer is 2 in standard arithmetic. Unless there's a specific context provided, that's the correct answer. I should present it clearly and maybe mention alternative contexts if they're relevant, but the primary answer is 2.\n",
      "</think>\n",
      "\n",
      "The sum of 1 + 1 in standard arithmetic (base 10) is **2**. \n",
      "\n",
      "However, depending on the context, it could have different interpretations:\n",
      "- **Binary (base 2):** 1 + 1 = 10 (which equals 2 in decimal).\n",
      "- **Modular arithmetic (e.g., modulo 2):** 1 + 1 = 0.\n",
      "- **Abstract algebra or set theory:** The union of two singleton sets has two elements, so 1 + 1 = 2.\n",
      "\n",
      "But **without additional context**, the default answer is **2**. \n",
      "\n",
      "**Answer:** 1 + 1 = **2**.\n",
      "Evaluation:\n",
      " Evaluation:\n",
      "\n",
      "**Clarity:** 9/10\n",
      "The answer provides a clear and detailed explanation of the different possible interpretations of the question \"What is 1+1 equal?\" The writer takes the time to consider various contexts, from basic arithmetic to abstract algebra and set theory. However, some parts of the reasoning are repetitive or rephrase the same idea multiple times.\n",
      "\n",
      "**Correctness:** 10/10\n",
      "The answer accurately presents different possible interpretations of the question, including standard arithmetic (2), binary (10), modular arithmetic (0), and abstract algebra/set theory (2). The writer acknowledges that without additional context, the default answer is indeed 2. There are no significant errors or inaccuracies.\n",
      "\n",
      "**Conciseness:** 7/10\n",
      "While the answer provides a thorough explanation of the possible interpretations, it could be more concise in presenting each point. Some ideas are repeated, and there are some unnecessary words/phrases that make the text slightly longer than necessary.\n",
      "\n",
      "Overall Score: 26/30\n",
      "\n",
      "Recommendation:\n",
      "The writer should focus on making their explanations more concise and clear. They can achieve this by removing redundant information, using bullet points or lists to present alternative interpretations, and providing a straightforward summary at the end.\n",
      "\n",
      "--- llama3 ---\n",
      "Response:\n",
      " The answer to 1+1 is... 2!\n",
      "Evaluation:\n",
      " **Clarity: 9/10**\n",
      "The answer is straightforward and easy to understand. However, a minor deduction for not explicitly stating the reasoning behind the answer.\n",
      "\n",
      "**Correctness: 10/10**\n",
      "The answer is correct, of course! The sum of 1+1 is indeed 2.\n",
      "\n",
      "**Conciseness: 8/10**\n",
      "While the answer is short and sweet, it would be even more concise if the reasoning was omitted or condensed into a single sentence. Nevertheless, the brevity is still commendable.\n",
      "\n",
      "**Overall Score: 27/30 = 9.0**\n",
      "\n",
      "I would give this answer a score of 9 out of 10. The only area for improvement is adding more detail to the reasoning behind the answer.\n",
      "\n",
      "--- gemma3 ---\n",
      "Response:\n",
      " 1 + 1 = 2\n",
      "\n",
      "It's a fundamental mathematical fact! \n",
      "\n",
      "Evaluation:\n",
      " Evaluation:\n",
      "\n",
      "Clarity: 9/10 (The answer is straightforward and easy to understand. However, the added phrase \"It's a fundamental mathematical fact!\" seems somewhat unnecessary and takes away from the clarity of the answer.)\n",
      "\n",
      "Correctness: 10/10 (The equation 1 + 1 = 2 is indeed correct in basic arithmetic.)\n",
      "\n",
      "Conciseness: 8/10 (While the answer is brief, it could be even more concise by simply stating \"1 + 1 = 2\" without the additional phrase. The score would increase if the unnecessary words were removed.)\n",
      "\n",
      "Reasoning:\n",
      "The equation 1 + 1 = 2 is a basic arithmetic fact that can be easily demonstrated through physical counting or simple addition. It's an elementary concept that requires no complex mathematical knowledge.\n",
      "\n",
      "Score: 8/10 (A good attempt, but could be improved with more concise language and reduced unnecessary phrases.)\n"
     ]
    }
   ],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     asyncio.run(main())\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379b154e-86ce-44dc-b0a8-45adb4b117cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
