{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2af5deb5-7ec8-43df-9bdf-3bc33d718bc6",
   "metadata": {},
   "source": [
    "# OLLAMA - Python Package Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b37e12e",
   "metadata": {},
   "source": [
    "> This page was generated from [ollama-interactive-inference/ollama-sif-py-ibex.ipynb](https://github.com/kaust-rccl/Data-science-onboarding/tree/main/notebooks/inference/ollama-interactive-inference/ollama-sif-py-ibex.ipynb). You can [view or download notebook](https://github.com/kaust-rccl/Data-science-onboarding/tree/main/notebooks/inference/ollama-interactive-inference/ollama-sif-py-ibex.ipynb). Or [view it on nbviewer](https://nbviewer.org/github/kaust-rccl/Data-science-onboarding/tree/main/notebooks/inference/ollama-interactive-inference/ollama-sif-py-ibex.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a5903c",
   "metadata": {},
   "source": [
    "## Objective\n",
    "In this notebook, we are going to use Ollama using [python package](https://github.com/ollama/ollama-python) approach \n",
    "\n",
    "## Initial Setup\n",
    "If you haven't installed conda yet, please follow [`How to Setup Conda on Ibex Guide`](https://docs.hpc.kaust.edu.sa/soft_env/prog_env/python_package_management/conda/ibex.html) to get started.\n",
    "\n",
    "After conda has been installed, save the following environment yaml file on Ibex under the name ``ollama_env.yaml``\n",
    "\n",
    "```yml\n",
    "name: ollama_env\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - _libgcc_mutex=0.1\n",
    "  - _openmp_mutex=4.5\n",
    "  - _python_abi3_support=1.0\n",
    "  - anyio=4.11.0\n",
    "  - argon2-cffi=25.1.0\n",
    "  - argon2-cffi-bindings=25.1.0\n",
    "  - arrow=1.4.0\n",
    "  - asttokens=3.0.0\n",
    "  - async-lru=2.0.5\n",
    "  - attrs=25.4.0\n",
    "  - babel=2.17.0\n",
    "  - beautifulsoup4=4.14.2\n",
    "  - bleach=6.2.0\n",
    "  - bleach-with-css=6.2.0\n",
    "  - brotli-python=1.1.0\n",
    "  - bzip2=1.0.8\n",
    "  - ca-certificates=2025.10.5\n",
    "  - cached-property=1.5.2\n",
    "  - cached_property=1.5.2\n",
    "  - certifi=2025.10.5\n",
    "  - cffi=2.0.0\n",
    "  - charset-normalizer=3.4.4\n",
    "  - comm=0.2.3\n",
    "  - cpython=3.14.0\n",
    "  - debugpy=1.8.17\n",
    "  - decorator=5.2.1\n",
    "  - defusedxml=0.7.1\n",
    "  - exceptiongroup=1.3.0\n",
    "  - executing=2.2.1\n",
    "  - fqdn=1.5.1\n",
    "  - h11=0.16.0\n",
    "  - h2=4.3.0\n",
    "  - hpack=4.1.0\n",
    "  - httpcore=1.0.9\n",
    "  - httpx=0.28.1\n",
    "  - hyperframe=6.1.0\n",
    "  - idna=3.11\n",
    "  - importlib-metadata=8.7.0\n",
    "  - ipykernel=7.0.1\n",
    "  - ipython=9.6.0\n",
    "  - ipython_pygments_lexers=1.1.1\n",
    "  - isoduration=20.11.0\n",
    "  - jedi=0.19.2\n",
    "  - jinja2=3.1.6\n",
    "  - json5=0.12.1\n",
    "  - jsonpointer=3.0.0\n",
    "  - jsonschema=4.25.1\n",
    "  - jsonschema-specifications=2025.9.1\n",
    "  - jsonschema-with-format-nongpl=4.25.1\n",
    "  - jupyter-lsp=2.3.0\n",
    "  - jupyter_client=8.6.3\n",
    "  - jupyter_core=5.9.1\n",
    "  - jupyter_events=0.12.0\n",
    "  - jupyter_server=2.17.0\n",
    "  - jupyter_server_terminals=0.5.3\n",
    "  - jupyterlab=4.4.9\n",
    "  - jupyterlab_pygments=0.3.0\n",
    "  - jupyterlab_server=2.27.3\n",
    "  - keyutils=1.6.3\n",
    "  - krb5=1.21.3\n",
    "  - lark=1.3.0\n",
    "  - ld_impl_linux-64=2.44\n",
    "  - libedit=3.1.20250104\n",
    "  - libexpat=2.7.1\n",
    "  - libffi=3.4.6\n",
    "  - libgcc=15.2.0\n",
    "  - libgcc-ng=15.2.0\n",
    "  - libgomp=15.2.0\n",
    "  - liblzma=5.8.1\n",
    "  - libmpdec=4.0.0\n",
    "  - libsodium=1.0.20\n",
    "  - libsqlite=3.50.4\n",
    "  - libstdcxx=15.2.0\n",
    "  - libstdcxx-ng=15.2.0\n",
    "  - libuuid=2.41.2\n",
    "  - libzlib=1.3.1\n",
    "  - markupsafe=3.0.3\n",
    "  - matplotlib-inline=0.1.7\n",
    "  - mistune=3.1.4\n",
    "  - nbclient=0.10.2\n",
    "  - nbconvert-core=7.16.6\n",
    "  - nbformat=5.10.4\n",
    "  - ncurses=6.5\n",
    "  - nest-asyncio=1.6.0\n",
    "  - notebook-shim=0.2.4\n",
    "  - openssl=3.5.4\n",
    "  - overrides=7.7.0\n",
    "  - packaging=25.0\n",
    "  - pandocfilters=1.5.0\n",
    "  - parso=0.8.5\n",
    "  - pexpect=4.9.0\n",
    "  - pickleshare=0.7.5\n",
    "  - pip=25.2\n",
    "  - platformdirs=4.5.0\n",
    "  - prometheus_client=0.23.1\n",
    "  - prompt-toolkit=3.0.52\n",
    "  - psutil=7.1.0\n",
    "  - ptyprocess=0.7.0\n",
    "  - pure_eval=0.2.3\n",
    "  - pycparser=2.22\n",
    "  - pygments=2.19.2\n",
    "  - pysocks=1.7.1\n",
    "  - python=3.14.0\n",
    "  - python-dateutil=2.9.0.post0\n",
    "  - python-fastjsonschema=2.21.2\n",
    "  - python-gil=3.14.0\n",
    "  - python-json-logger=2.0.7\n",
    "  - python-tzdata=2025.2\n",
    "  - python_abi=3.14\n",
    "  - pytz=2025.2\n",
    "  - pyyaml=6.0.3\n",
    "  - pyzmq=27.1.0\n",
    "  - readline=8.2\n",
    "  - referencing=0.37.0\n",
    "  - requests=2.32.5\n",
    "  - rfc3339-validator=0.1.4\n",
    "  - rfc3986-validator=0.1.1\n",
    "  - rfc3987-syntax=1.1.0\n",
    "  - rpds-py=0.27.1\n",
    "  - send2trash=1.8.3\n",
    "  - setuptools=80.9.0\n",
    "  - six=1.17.0\n",
    "  - sniffio=1.3.1\n",
    "  - soupsieve=2.8\n",
    "  - stack_data=0.6.3\n",
    "  - terminado=0.18.1\n",
    "  - tinycss2=1.4.0\n",
    "  - tk=8.6.13\n",
    "  - tomli=2.3.0\n",
    "  - tornado=6.5.2\n",
    "  - traitlets=5.14.3\n",
    "  - typing-extensions=4.15.0\n",
    "  - typing_extensions=4.15.0\n",
    "  - typing_utils=0.1.0\n",
    "  - tzdata=2025b\n",
    "  - uri-template=1.3.0\n",
    "  - urllib3=2.5.0\n",
    "  - wcwidth=0.2.14\n",
    "  - webcolors=24.11.1\n",
    "  - webencodings=0.5.1\n",
    "  - websocket-client=1.9.0\n",
    "  - yaml=0.2.5\n",
    "  - zeromq=4.3.5\n",
    "  - zipp=3.23.0\n",
    "  - zstandard=0.25.0\n",
    "  - zstd=1.5.7\n",
    "  - pip:\n",
    "      - annotated-types==0.7.0\n",
    "      - ollama==0.6.0\n",
    "      - pydantic==2.12.3\n",
    "      - pydantic-core==2.41.4\n",
    "      - typing-inspection==0.4.2\n",
    "```\n",
    "\n",
    "Run the following command to build the conda environment:\n",
    "```bash\n",
    "conda env create -f ollama_env.yaml\n",
    "```\n",
    "\n",
    "## Starting JupyterLab\n",
    "Follow [`Guide: Using_Jupyter on Ibex`](https://docs.hpc.kaust.edu.sa/soft_env/job_schd/slurm/interactive_jobs/jupyter.html#job-on-ibex) to start JupyterLab on a an Ibex GPU node using your conda environment instead of *'machine_learning'* module.\n",
    "\n",
    "By making the following changes to the Jupyter launch script:\n",
    "```bash\n",
    "#module load machine_learning/2024.01\n",
    "conda activate ollama_env\n",
    "```\n",
    "\n",
    "## Starting The Ollama Server\n",
    "Start the OLLAMA REST API server using the following bash script in a terminal:\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "# Cleanup process while exiting the server\n",
    "cleanup() {\n",
    "    echo \"ðŸ§¹   Cleaning up before exit...\"\n",
    "    # Put your exit commands here, e.g.:\n",
    "    rm -f $OLLAMA_PORT_TXT_FILE\n",
    "    # Remove the Singularity instance\n",
    "    singularity instance stop $SINGULARITY_INSTANCE_NAME\n",
    "}\n",
    "trap cleanup SIGINT  # Catch Ctrl+C (SIGINT) and run cleanup\n",
    "\n",
    "# User Editable Section\n",
    "# 1. Make target directory on /ibex/user/$USER/ollama_models_scratch to store your Ollama models\n",
    "export OLLAMA_MODELS_SCRATCH=/ibex/user/$USER/ollama_models_scratch\n",
    "mkdir -p $OLLAMA_MODELS_SCRATCH\n",
    "# End of User Editable Section\n",
    "\n",
    "SINGULARITY_INSTANCE_NAME=\"ollama\"\n",
    "OLLAMA_PORT_TXT_FILE='ollama_port.txt'\n",
    "\n",
    "# 2. Load Singularity module\n",
    "module load singularity\n",
    "\n",
    "# 3. Pull OLLAMA docker image\n",
    "singularity pull docker://ollama/ollama\n",
    "\n",
    "# 4. Change the default port for OLLAMA_HOST: (default 127.0.0.1:11434)\n",
    "export PORT=$(python -c 'import socket; s=socket.socket(); s.bind((\"\", 0)); print(s.getsockname()[1]); s.close()')\n",
    "\n",
    "# 5. Copy the assigned port, it will be required in the second part during working on the notebook.\n",
    "echo \"$PORT\" > $OLLAMA_PORT_TXT_FILE\n",
    "\n",
    "echo \"OLLAMA PORT: $PORT  -- Stored in $OLLAMA_PORT_TXT_FILE\"\n",
    "\n",
    "# 6. Define the OLLAMA Host\n",
    "export SINGULARITYENV_OLLAMA_HOST=127.0.0.1:$PORT\n",
    "\n",
    "# 7. Change the default model directory stored: (default ~/.ollama/models/manifests/registry.ollama.ai/library)\n",
    "export SINGULARITYENV_OLLAMA_MODELS=$OLLAMA_MODELS_SCRATCH\n",
    "\n",
    "# 8. Create an Instance:\n",
    "singularity instance start --nv -B \"/ibex/user:/ibex/user\" ollama_latest.sif $SINGULARITY_INSTANCE_NAME\n",
    "\n",
    "# 7. Run the OLLAMA REST API server on the background\n",
    "singularity exec instance://$SINGULARITY_INSTANCE_NAME bash -c \"ollama serve\"\n",
    "```\n",
    "\n",
    "> Note: Save the above script in a file called start_ollama_server.sh\n",
    "\n",
    "```bash\n",
    "# Run the script to start the Ollama server.\n",
    "bash start_ollama_server.sh\n",
    "```\n",
    "\n",
    "The script has the following:\n",
    "- A user editable section, where the user defines Ollama models scratch directory.\n",
    "- The allocated port is saved in a temporary ollama_port.txt file, in order to be used in the Python notebook to read the assigned port to Ollama server.\n",
    "- Cleanup section in order to stop the singularity instance when the script is terminated with CTRL+C.\n",
    "\n",
    "## Using REST API Requests\n",
    "Follow the following Python notebook below, it contains the codes for:\n",
    "- Initialization Setup.\n",
    "- List local models.\n",
    "- Pull models.\n",
    "- Testing connection to the Ollama server.\n",
    "- Chat with the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbd6d4e",
   "metadata": {},
   "source": [
    "### 1. Initialization\n",
    "1. Define the base URL for the remote Ollama Server.\n",
    "2. Create a connection Object to talk to the Ollama server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5eed861-4b17-4e2b-9a06-8331318cc42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://127.0.0.1:49313\n"
     ]
    }
   ],
   "source": [
    "# 1.1- Define the base URL for the remote Ollama Server.\n",
    "with open(\"ollama_port.txt\") as f :\n",
    "    PORT = f.read().strip()\n",
    "BASE_URL=f\"http://127.0.0.1:{PORT}\"\n",
    "print(BASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49c5373d-ab52-4bf1-8639-c2d753c684ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create a connection Object to talk to the Ollama server\n",
    "from ollama import Client\n",
    "\n",
    "# Create a client instance\n",
    "client = Client(\n",
    "  host=BASE_URL,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2e227c-803e-4f3b-a01a-e54c8d27ee0a",
   "metadata": {},
   "source": [
    "### 2. Get a List of Local Models\n",
    "- Get a list of locally available Ollama models.\n",
    "- Locally available models are located under path: */ibex/user/$USER/ollama_models_scratch*\n",
    "- To change the location for pulled models, modify the variable *OLLAMA_MODELS_SCRATCH* in the script*start_ollama_server.sh*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25fc7510-856b-4a2e-a275-a8f5660edc8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gemma3:270m', 'qwen3:0.6b']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_local_models():\n",
    "    \"\"\"\n",
    "    Returns a list of locally available Ollama Models.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of model names as strings\n",
    "    \"\"\"\n",
    "    models = [model['model'] for model in client.list()['models']]\n",
    "    return models\n",
    "\n",
    "get_local_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df78d5ba-d8a6-4239-a50a-2c357bcee143",
   "metadata": {},
   "source": [
    "### 3. Pull The Model\n",
    "- To pull a specific model, use *pull* method.\n",
    "- Please refer to [Ollama Library](https://ollama.com/library) to check available models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97bd9c31-221a-4efa-a89c-6ba496f866fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProgressResponse(status='success', completed=None, total=None, digest=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pull the required models\n",
    "client.pull(\"gemma3:270m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d072bf1-bd45-4977-871e-885c983e2808",
   "metadata": {},
   "source": [
    "### 4. Running a sample query\n",
    "#### 4.1- Non-Streaming Request\n",
    "- Sends the full message to the model and waits for the complete response.\n",
    "- The function returns only after the model finishes generating.\n",
    "- Simple and easy to use\n",
    "- Slower perceived latency â€” nothing is shown until the answer is complete.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f267a07-1425-48f8-8aad-8ec8236c05fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The sky is blue because of a phenomenon called **Rayleigh Scattering**. Here's a breakdown of why:\\n\\n*   **Light and Colors:** Light is made up of all the colors of the visible spectrum.\\n*   **Wavelength:** The color of light is inversely proportional to its wavelength. The shorter the wavelength, the more energy is absorbed by the light.\\n*   **Absorption and Scattering:** When light hits an object, it can be scattered in different directions. This scattering is caused by the particles of the object (like dust, molecules, and stars).\\n*   **Blue Light:** Blue light has a shorter wavelength than other colors. This means that it has a higher energy and can be scattered more effectively.\\n*   **Why Blue is Important:** Blue light is what we see because it's the most energetic part of the electromagnetic spectrum. It's why the sky appears blue.\\n\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the target LLM model\n",
    "model = 'gemma3:270m'\n",
    "\n",
    "response = client.chat(model=model, messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Why is the sky blue?',\n",
    "  },\n",
    "])\n",
    "response['message']['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08df0ec-b3ab-46ec-b30d-b35b19d8d947",
   "metadata": {},
   "source": [
    "#### 4.2- Streaming Request (Synchronous)\n",
    "- Requests the model to stream its output as itâ€™s generated.\n",
    "- Each 'chunk' contains a partial piece of the message.\n",
    "- Ideal for real-time display or CLI tools\n",
    "- Still blocks your main thread while waiting for new chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76b0b12d-cfb2-47ca-9cae-1ac42ebb8a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sky is blue due to a phenomenon called **Rayleigh scattering**. Here's a breakdown of why:\n",
      "\n",
      "*   **Sunlight:** Sunlight is made up of all the colors of the rainbow.\n",
      "*   **Antennaction:** The sun's rays are bent by the Earth's rotation.\n",
      "*   **Scattering:** As sunlight passes through the atmosphere, the light interacts with the air molecules. This scattering is much more effective at scattering shorter wavelengths of light (blue and violet) than longer wavelengths (red and orange).\n",
      "*   **Blue Light:** Blue light has a shorter wavelength and is scattered more strongly than other colors.\n",
      "*   **Why Blue?** Because blue light is scattered more than other colors, making the sky appear blue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the target LLM model\n",
    "model = 'gemma3:270m'\n",
    "\n",
    "stream = client.chat(\n",
    "    model=model,\n",
    "    messages=[{'role': 'user', 'content': 'Why is the sky blue?'}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "  print(chunk['message']['content'], end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27062c7e-3293-4ead-b969-0c38d9bd56d3",
   "metadata": {},
   "source": [
    "#### 4.3- Asynchronous Streaming Chat\n",
    "- Same as streaming mode, but runs inside an async event loop.\n",
    "- Allows other async tasks to run concurrently while receiving outputs.\n",
    "- Best for Jupyter notebooks, web servers, or multitasking apps\n",
    "- Requires 'await' and async context to run properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9dd39d0b-de1f-47ce-a217-e470f264351a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sky is blue due to a phenomenon called **Rayleigh scattering**. \n",
      "\n",
      "Here's a breakdown:\n",
      "\n",
      "*   **Light travels in waves:** When light hits a surface, it diffracts (spreads out) in all directions.\n",
      "*   **Wavelength:** The wavelength of light is inversely proportional to its frequency. A longer wavelength means a shorter frequency, and vice versa.\n",
      "*   **Scattering:** As the light travels, it bumps into tiny particles in the air. These particles are called atoms or molecules.\n",
      "*   **Rayleigh Scattering:** When these particles collide with the atoms, they scatter the light in all directions. This scattering is much more effective at scattering shorter wavelengths of light (blue and violet) than longer wavelengths (red and orange).\n",
      "*   **Why Blue?** Because blue light has a shorter wavelength than other colors, and therefore is scattered more effectively by the atoms in the air. This scattering is what gives the sky its blue color.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from ollama import AsyncClient\n",
    "\n",
    "async def chat_with_model(model: str, prompt: str, base_url: str = BASE_URL):\n",
    "    \"\"\"\n",
    "    Stream a chat response from a local Ollama model for a given prompt.\n",
    "\n",
    "    Args:\n",
    "        model (str): Name of the local Ollama model to use.\n",
    "        prompt (str): The user input to send to the model.\n",
    "        base_url (str, optional): The host URL for the Ollama server. Defaults to BASE_URL.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the requested model is not in the local models list.\n",
    "    \"\"\"\n",
    "    # Validate model existence\n",
    "    if model not in get_local_models():\n",
    "        raise ValueError(f\"Requested model '{model}' is not in the local list. Pull the model first!\")\n",
    "        \n",
    "    message = {'role': 'user', 'content': prompt}\n",
    "\n",
    "    client = AsyncClient(host=base_url)\n",
    "    async for part in await client.chat(model=model, messages=[message], stream=True):\n",
    "        print(part['message']['content'], end='', flush=True)\n",
    "\n",
    "\n",
    "# Usage\n",
    "model = 'gemma3:270m'\n",
    "await chat_with_model(model=model, prompt=\"Why the sky is blue?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f837db-a07b-4d5b-9b50-df34116f3b01",
   "metadata": {},
   "source": [
    "### 5- Interactive Chat with Ollama Models\n",
    "- This function enables a live, interactive conversation with a local Ollama LLM model.\n",
    "- Users can type messages in the terminal, and the model streams its responses in real time.\n",
    "- Features:\n",
    "    - Maintains conversation history between user and model.\n",
    "    - Supports multiple local models (must be pulled beforehand).\n",
    "    - Type 'exit' or 'quit' to end the session.\n",
    "    - Returns the full conversation history for further processing or logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57dfbe22-0c28-4578-a6c0-16bdeb035bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from ollama import AsyncClient\n",
    "\n",
    "# Stores full conversation history\n",
    "messages = []\n",
    "\n",
    "async def interactive_chat(model: str, base_url: str = BASE_URL, history: list = None):\n",
    "    \"\"\"\n",
    "    Start an interactive chat session with a local Ollama model.\n",
    "\n",
    "    This function streams responses from the model in real time,\n",
    "    maintaining conversation history. Users can type 'exit' or 'quit'\n",
    "    to end the session.\n",
    "\n",
    "    Args:\n",
    "        model (str): Name of the local Ollama model to use.\n",
    "        base_url (str, optional): Host URL for the Ollama server. Defaults to BASE_URL.\n",
    "        history (list, optional): Pre-existing conversation history. Defaults to a new list.\n",
    "\n",
    "    Returns:\n",
    "        list: Full conversation history as a list of message dictionaries.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the requested model is not in the local models list.\n",
    "    \"\"\"\n",
    "    # Validate model existence\n",
    "    if model not in get_local_models():\n",
    "        raise ValueError(f\"Requested model '{model}' is not in the local list. Pull the model first!\")\n",
    "\n",
    "    if history is None:\n",
    "        history = []\n",
    "        \n",
    "    client = AsyncClient(host=base_url)\n",
    "    print(\"ðŸ¤– Chat started â€” type 'exit' to quit.\\n\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"ðŸ‘¤ You: \")\n",
    "        if user_input.lower().strip() in {\"exit\", \"quit\"}:\n",
    "            print(\"ðŸ‘‹ Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Add user input to history\n",
    "        history.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "        print(\"ðŸ¤– Ollama:\", end=\" \", flush=True)\n",
    "        assistant_reply = \"\"\n",
    "\n",
    "        async for chunk in await client.chat(\n",
    "            model=model,\n",
    "            messages=history,\n",
    "            stream=True\n",
    "        ):\n",
    "            if chunk.get(\"message\"):\n",
    "                part = chunk[\"message\"][\"content\"]\n",
    "                print(part, end='', flush=True)\n",
    "                assistant_reply += part\n",
    "\n",
    "        print(\"\\n\")  # Newline after full reply\n",
    "\n",
    "        # Add assistant reply to history\n",
    "        history.append({\"role\": \"assistant\", \"content\": assistant_reply})\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19b088e1-da2e-40f0-b3a6-680b541d582d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Chat started â€” type 'exit' to quit.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ðŸ‘¤ You:  Hello, Explain what is HPC?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Ollama: HPC stands for **High-Performance Computing**. It's a field of computer science focused on designing and implementing computing systems that can handle a large number of concurrent tasks and data efficiently.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ðŸ‘¤ You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘‹ Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "model = 'gemma3:270m'\n",
    "history = await interactive_chat(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab594ae-1149-4c9a-98b6-25adc60d058f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
